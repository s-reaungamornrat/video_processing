{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3268622f-8b56-4b75-afc0-c245cd30fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import boxes as box_ops\n",
    "from torchvision.ops.boxes import box_area\n",
    "from torchvision.ops.roi_align import _bilinear_interpolate, roi_align\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from video_processing.mask_rcnn.dataset.penn_fudan_ped import PennFudanDataset, get_transform\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21762928-d9d5-4cf8-aa52-f5cd5b862b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(rpn_objectness_weight=10.0, rpn_box_regression_weight=50.0, roi_classifier_weight=1.0, roi_box_regression_weight=5.0, mask_weight=0.5, n_training=160, max_epochs=4, print_freq=5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description='MaskRCNN')\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--rpn-objectness-weight', type=float, default=10., help='weight to the loss of RPN objectness')\n",
    "parser.add_argument('--rpn-box-regression-weight', type=float, default=50., help='weight to the loss of RPN box regression')\n",
    "parser.add_argument('--roi-classifier-weight', type=float, default=1., help='weight to the loss of roi-head for object class')\n",
    "parser.add_argument('--roi-box-regression-weight', type=float, default=5., help='weight to the loss for roi-head box regression')\n",
    "parser.add_argument('--mask-weight', type=float, default=0.5, help='weight to mask loss')\n",
    "parser.add_argument('--n-training', type=int, default=160, help='number of training data')\n",
    "parser.add_argument('--max-epochs', type=int, default=160, help='maximum number of epochs')\n",
    "parser.add_argument('--print-freq', type=int, default=5, help='how many batch before printing')\n",
    "\n",
    "args = parser.parse_args('--max-epochs 4'.split())\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e314737-1208-4edb-86e2-187e0577af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device ', device)\n",
    "\n",
    "main_dirpath='D:/data/'\n",
    "tensor_dirpath=os.path.join(main_dirpath, 'mask_rcnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177d3d09-c269-459c-84a8-758c16318d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url='https://raw.githubusercontent.com/pytorch/vision/main/references/detection'\n",
    "for file in ['engine.py', 'utils.py','coco_utils.py','coco_eval.py', 'transforms.py']:\n",
    "    if os.path.isfile(file): continue\n",
    "    out=wget.download(url=f'{main_url}/{file}', out=None)\n",
    "    print(file, out)\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d090cc-d44d-4055-8be2-75f3316eb51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of inpute features for classifiers  1024\n",
      "the number of input features for mask  256\n"
     ]
    }
   ],
   "source": [
    "num_classes=2\n",
    "# load an instance segmentation model pre-trained on COCO\n",
    "model=torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='DEFAULT', rpn_pre_nms_top_n_train=800,\n",
    "        rpn_pre_nms_top_n_test=500,  rpn_post_nms_top_n_train=800,  rpn_post_nms_top_n_test=500, box_detections_per_img=100,\n",
    "        box_batch_size_per_image =400, rpn_batch_size_per_image=100)\n",
    "# get number of input features for the classifier\n",
    "in_features=model.roi_heads.box_predictor.cls_score.in_features\n",
    "print('the number of inpute features for classifiers ', in_features)\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor=FastRCNNPredictor(in_features,  num_classes)\n",
    "\n",
    "# get the number of input features for the mask classifiers\n",
    "in_features_mask=model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "print('the number of input features for mask ', in_features_mask)\n",
    "hidden_layer=256\n",
    "# and replace the mask predictor with a new one\n",
    "model.roi_heads.mask_predictor=MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device);\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2255bc2-46d3-40f6-8a3e-cb22a4576063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset  160  val_dataset  10\n"
     ]
    }
   ],
   "source": [
    "# there are 170 images\n",
    "train_dataset=PennFudanDataset(root=os.path.join(main_dirpath, 'PennFudanPed'), image_dirname='PNGImages', mask_dirname='PedMasks',\n",
    "                               annotation_dirname='Annotation', transforms=get_transform(is_train=True), indices=np.arange(160))\n",
    "val_dataset=PennFudanDataset(root=os.path.join(main_dirpath, 'PennFudanPed'), image_dirname='PNGImages', mask_dirname='PedMasks',\n",
    "                               annotation_dirname='Annotation', transforms=get_transform(is_train=False), indices=np.arange(160,170))\n",
    "print('train_dataset ', len(train_dataset), ' val_dataset ', len(val_dataset))\n",
    "# define training and validation data loaders\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset, batch_size=2,shuffle=True,collate_fn=utils.collate_fn)\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset, batch_size=2,shuffle=False,collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1459368d-c04f-47ae-98c5-af84a8230b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer,weights, print_freq=1):\n",
    "    \n",
    "    running_loss=0.\n",
    "    for i, (images, targets) in enumerate(train_loader, 1):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # estimate\n",
    "        outputs=model(images, targets)\n",
    "    \n",
    "        # compute loss\n",
    "        loss, log=0., ''\n",
    "        for name, loss_term in outputs.items():\n",
    "            loss+=weights[name]*loss_term\n",
    "            log+=f' {name}:{loss.item():.3f},'\n",
    "        loss.backward()\n",
    "    \n",
    "        # adjusting parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        # gather data and report\n",
    "        running_loss+=loss.item()\n",
    "        if print_freq>0 and i%print_freq==0:\n",
    "            print(f'Iteration {i} [{100*i/len(train_loader):2f}%]: total:{loss.item():.3f}, {log}')\n",
    "    last_loss=running_loss/len(train_loader)\n",
    "    return last_loss\n",
    "\n",
    "def validation(model, val_loader, weights):\n",
    "    running_loss, loss_per_task=0., dict()\n",
    "    for i, (images, targets) in enumerate(val_loader, 1):\n",
    "        with torch.no_grad(): outputs=model(images, targets)\n",
    "        # compute loss\n",
    "        loss=0\n",
    "        for name, loss_term in outputs.items():\n",
    "            loss+=weights[name]*loss_term\n",
    "            if name not in loss_per_task: loss_per_task[name]=0.\n",
    "            loss_per_task[name]+=loss_term.item()\n",
    "        running_loss+=loss.item()\n",
    "        break\n",
    "    loss/=len(val_loader)\n",
    "    loss_per_task={k:n/len(val_loader) for k, n in loss_per_task.items()}\n",
    "    return loss, loss_per_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7ad78-ba9b-4d8f-ba12-68041fe8bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={'loss_classifier':args.roi_classifier_weight, 'loss_box_reg':args.roi_box_regression_weight, \n",
    "         'loss_mask':args.mask_weight,'loss_objectness':args.rpn_objectness_weight, \n",
    "         'loss_rpn_box_reg':args.rpn_box_regression_weight}\n",
    "\n",
    "model.train()\n",
    "best_vloss=np.inf\n",
    "for epoch in range(1, args.max_epochs+1):\n",
    "    # make sure gradient tracking is on\n",
    "    t_loss=train_epoch(model, train_loader, optimizer, weights=weights, print_freq=args.print_freq)\n",
    "\n",
    "    # validation\n",
    "    v_loss, v_loss_per_task=validation(model, val_loader, weights)\n",
    "\n",
    "    # print progress\n",
    "    log=''\n",
    "    for k, v in v_loss_per_task.items(): log+=f' {k}:{v:.2f},'\n",
    "    print(f'Epoch {epoch} [{epoch/args.max_epochs}:.2f]: training loss {t_loss:.3f}, val_loss: {v_loss:.3f}, {log}')\n",
    "\n",
    "    if v_loss<best_vloss and epoch>1:\n",
    "        best_vloss=v_loss\n",
    "        torch.save(model.state_dict(), f'checkpoint{epoch}.pth')\n",
    "        \n",
    "    torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94c5c526-5d93-4e31-9b8f-df92f35c03ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('checkpoint.pth', weights_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
