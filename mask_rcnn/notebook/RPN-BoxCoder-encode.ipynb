{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda3a191-d442-4fc2-b7de-64a14f81a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42982173-298a-4b25-bba9-684d30c96eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched_gt_boxes  <class 'list'> [torch.Size([214830, 4]), torch.Size([214830, 4])]\n",
      "anchors  <class 'list'> [torch.Size([214830, 4]), torch.Size([214830, 4])]\n"
     ]
    }
   ],
   "source": [
    "data_dirpath='D:/data/mask_rcnn'\n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "box_coder_encoder=torch.load(os.path.join(data_dirpath, \"box_coder_encoder.pt\"),map_location=device, weights_only=True)\n",
    "matched_gt_boxes=box_coder_encoder['matched_gt_boxes']\n",
    "anchors=box_coder_encoder['anchors']\n",
    "print('matched_gt_boxes ', type(matched_gt_boxes), [m.shape for m in matched_gt_boxes])\n",
    "print('anchors ', type(anchors), [a.shape for a in anchors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294d0f21-b6b5-485e-ad02-3ba44dae4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes_per_image  [214830, 214830]\n",
      "reference_boxes  torch.Size([429660, 4]) tensor([ 38.,  21., 114., 298.]) tensor([248., 178., 369., 435.])\n",
      "proposals  torch.Size([429660, 4]) tensor([-362., -362.,   11.,   11.]) tensor([ 881.,  945., 1194., 1258.])\n"
     ]
    }
   ],
   "source": [
    "# regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)\n",
    "reference_boxes=matched_gt_boxes\n",
    "proposals=anchors\n",
    "\n",
    "boxes_per_image=[len(b) for b in reference_boxes]\n",
    "print('boxes_per_image ', boxes_per_image)\n",
    "reference_boxes=torch.cat(reference_boxes, dim=0)\n",
    "print('reference_boxes ', reference_boxes.shape, reference_boxes.min(0).values,\n",
    "     reference_boxes.max(0).values)\n",
    "proposals=torch.cat(proposals, dim=0)\n",
    "print('proposals ', proposals.shape, proposals.min(0).values, proposals.max(0).values)\n",
    "# targets = self.encode_single(reference_boxes, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9bef2f9-ebaa-4e83-896a-d4451ef6e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposals_y1  torch.Size([429660, 1])\n",
      "proposals_y2  torch.Size([429660, 1])\n",
      "reference_boxes_x1  torch.Size([429660, 1])\n",
      "reference_boxes_x2  torch.Size([429660, 1])\n"
     ]
    }
   ],
   "source": [
    "device=reference_boxes.device\n",
    "dtype=reference_boxes.dtype\n",
    "weights=(1.0, 1.0, 1.0, 1.0) # weights for x, y, w, h\n",
    "weights=torch.as_tensor(weights, device=device, dtype=dtype)\n",
    "# targets = encode_boxes(reference_boxes, proposals, weights)\n",
    "\n",
    "# perform some unpacking to make it JIT-fusion friendly\n",
    "wx=weights[0]\n",
    "wy=weights[1]\n",
    "ww=weights[2]\n",
    "wh=weights[3]\n",
    "\n",
    "proposals_x1=proposals[:,0].unsqueeze(1)\n",
    "proposals_y1=proposals[:,1].unsqueeze(1)\n",
    "proposals_x2=proposals[:,2].unsqueeze(1)\n",
    "proposals_y2=proposals[:,3].unsqueeze(1)\n",
    "\n",
    "reference_boxes_x1=reference_boxes[:,0].unsqueeze(1)\n",
    "reference_boxes_y1=reference_boxes[:,1].unsqueeze(1)\n",
    "reference_boxes_x2=reference_boxes[:,2].unsqueeze(1)\n",
    "reference_boxes_y2=reference_boxes[:,3].unsqueeze(1)\n",
    "\n",
    "print('proposals_y1 ', proposals_y1.shape)\n",
    "print('proposals_y2 ', proposals_y2.shape)\n",
    "print('reference_boxes_x1 ', reference_boxes_x1.shape)\n",
    "print('reference_boxes_x2 ', reference_boxes_x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9355c0-fa8d-4dd4-ad39-4d6ab9c14a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation starts here\n",
    "ex_widths=proposals_x2-proposals_x1\n",
    "ex_heights=proposals_y2-proposals_y1\n",
    "ex_ctr_x=proposals_x1+0.5*ex_widths # center in x\n",
    "ex_ctr_y=proposals_y1+0.5*ex_heights # center in y\n",
    "\n",
    "gt_widths=reference_boxes_x2-reference_boxes_x1\n",
    "gt_heights=reference_boxes_y2-reference_boxes_y1\n",
    "gt_ctr_x=reference_boxes_x1+0.5*gt_widths\n",
    "gt_ctr_y=reference_boxes_y1+0.5*gt_heights\n",
    "\n",
    "targets_dx=wx*(gt_ctr_x-ex_ctr_x)/ex_widths\n",
    "targets_dy=wy*(gt_ctr_y-ex_ctr_y)/ex_heights\n",
    "targets_dw=ww*torch.log(gt_widths/ex_widths)\n",
    "targets_dh=wh*torch.log(gt_heights/ex_heights)\n",
    "\n",
    "targets=torch.cat((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_cv",
   "language": "python",
   "name": "op_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
