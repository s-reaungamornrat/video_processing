{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad03a53-3871-433a-b3a1-d18d4466635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea5af725-1ed2-485d-b01a-66823bc84d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxCoder:\n",
    "    \"\"\"\n",
    "    This class encodes and decodes a set of bounding boxes into the representation used for training the regressors\n",
    "    From https://github.com/pytorch/vision/blob/main/torchvision/models/detection/_utils.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inverse_weights, bbox_xform_clip=math.log(1000.0 / 16)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inverse_weights (4-element tuple): used to scale down predicted deltas, typically set to 1 for all elements, \n",
    "                i.e., to be applied as 1/inverse_weights\n",
    "            bbox_xform_clip (float)\n",
    "        \"\"\"\n",
    "        self.inverse_weights=inverse_weights\n",
    "        self.bbox_xform_clip=bbox_xform_clip\n",
    "\n",
    "    def decode(self, pred_deltas, anchors):\n",
    "        \"\"\"\n",
    "        Determine predicted bounding box locations based on predicted deltas and anchors. Predicted deltas applied as \n",
    "        the scales of width and height of anchor boxes to determine the offsets along x and y, and the width and height of\n",
    "        the predicted boxes. \n",
    "        Args:\n",
    "            pred_deltas (tensor): Nx4 float deltas/scales of x-offset, y-offset, width, and height where \n",
    "                N=BHWA for B batch size, H height, W width, and A anchors (where number of anchors=number of aspect ratios)\n",
    "            anchors (list): list of B tensors representing anchors per image, each tensor of size HWAx4\n",
    "        Return:\n",
    "            pred_boxes (tensor): Nx1x4 or BHWAx1x4 predicted bounding box corner locations, where each row is x1,y1,x2,y2\n",
    "        \"\"\"\n",
    "        assert isinstance(anchors, (list, tuple)), 'This function expects anchors of type list or tuple'\n",
    "        assert isinstance(pred_deltas, torch.Tensor), 'This function expects pred_deltas of type torch.Tensor'\n",
    "        anchors_per_images=[a.size(0) for a in anchors]\n",
    "        concat_anchors=torch.cat(anchors, dim=0) # making the anchors the same size as pred_deltas -> BHWAx4\n",
    "        anchors_sum=sum(anchors_per_images)\n",
    "        print('anchors_per_images ', anchors_per_images, ' concat_anchors ', concat_anchors.shape, ' anchors_sum ', anchors_sum)\n",
    "        if anchors_sum>0: pred_deltas=pred_deltas.reshape(anchors_sum, -1)\n",
    "        print('pred_deltas ', pred_deltas.shape)\n",
    "        pred_boxes=self.__decode__(pred_deltas=pred_deltas, anchors=concat_anchors)\n",
    "        if anchors_sum>0: pred_boxes=pred_boxes.reshape(anchors_sum, -1, 4)\n",
    "        return pred_boxes\n",
    "\n",
    "    def __decode__(self, pred_deltas, anchors):\n",
    "        \"\"\"\n",
    "        Determine predicted bounding box locations based on predicted deltas and anchors. Predicted deltas applied as \n",
    "        the scales of width and height of anchor boxes to determine the offsets along x and y, and the width and height of\n",
    "        the predicted boxes. \n",
    "        Args:\n",
    "            pred_deltas (tensor): Nx4 float deltas/scales of x-offset, y-offset, width, and height where \n",
    "                N=BHWA for B batch size, H height, W width, and A anchors (where number of anchors=number of aspect ratios)\n",
    "            anchors (tensor): Nx4 float deltas/scales of x-offset, y-offset, width, and height where \n",
    "                N=BHWA for B batch size, H height, W width, and A anchors (where number of anchors=number of aspect ratios)\n",
    "        Return:\n",
    "            pred_boxes (tensor): Nx4 or BHWAx4 predicted bounding box corner locations, where each row is x1,y1,x2,y2\n",
    "        \"\"\"\n",
    "        # make sure that anchors and predicted_deltas are of the same type\n",
    "        anchors=anchors.type(pred_deltas.dtype)\n",
    "        \n",
    "        # elements in each row of anchors (refence boxes) are x1,y1,x2,y2 where x1<=x2 and y1<=y2 \n",
    "        # we compute the width and height of refence boxes\n",
    "        width=anchors[:, 2]-anchors[:, 0]  # 1D tensor of size N\n",
    "        height=anchors[:, 3]-anchors[:, 1]  # 1D tensor of size N\n",
    "        # then we compute the center of the reference boxes\n",
    "        ctr_x=anchors[:, 0]+ 0.5*width  # 1D tensor of size N, this is the center\n",
    "        ctr_y=anchors[:, 1]+ 0.5*height  # 1D tensor of size N, this is the center\n",
    "        \n",
    "        wx, wy, ww, wh=self.inverse_weights\n",
    "        # we weights the delta estimates down\n",
    "        # Nx1 dx here is delta scale relative to width for offset along x (e.g., offset from center along x)\n",
    "        dx=pred_deltas[:, 0::4]/wx # similar to pred_deltas[:, 0].unsqueeze(-1) for x/wx\n",
    "        # Nx1 dy here is delta scale relative to height for offset along y (e.g., offset from center along y)\n",
    "        dy=pred_deltas[:, 1::4]/wy # similar to pred_deltas[:, 1].unsqueeze(-1) for y/wy\n",
    "        # Nx1 dw is scale in log-space of width (i.e., apply exp before scaling width)\n",
    "        dw=pred_deltas[:, 2::4]/ww # similar to pred_deltas[:, 2].unsqueeze(-1) for w/ww\n",
    "        # Nx1 dh is scale in log-space of height (i.e., apply exp before scaling height)\n",
    "        dh=pred_deltas[:, 3::4]/wh # similar to pred_deltas[:, 3].unsqueeze(-1) for h/wh\n",
    "        print('dx ', dx.shape, dx.min().item(), dx.max().item())\n",
    "        print('dy ', dy.shape, dy.min().item(), dy.max().item())\n",
    "        print('dw ', dw.shape, dw.min().item(), dw.max().item())\n",
    "        print('dh ', dh.shape, dh.min().item(), dh.max().item())\n",
    "        \n",
    "        # prevent sending too large values to torch.exp\n",
    "        dw=torch.clamp(dw, max=self.bbox_xform_clip)\n",
    "        dh=torch.clamp(dh, max=self.bbox_xform_clip)\n",
    "        \n",
    "        # Nx1 = Nx1 Nx1 Nx1\n",
    "        pred_ctr_x=dx*width[:,None]+ctr_x[:,None]\n",
    "        pred_ctr_y=dy*height[:,None]+ctr_y[:,None]\n",
    "        pred_w=torch.exp(dw)*width[:,None]\n",
    "        pred_h=torch.exp(dh)*height[:,None]\n",
    "        \n",
    "        # Nx1 distance from boundary to the box's center. We use these to determine the bounding box\n",
    "        # corner location, x1,y1,x2,y2\n",
    "        c_to_c_h = torch.tensor(0.5, dtype=pred_ctr_y.dtype, device=pred_h.device)*pred_h\n",
    "        c_to_c_w = torch.tensor(0.5, dtype=pred_ctr_x.dtype, device=pred_w.device)*pred_w\n",
    "        \n",
    "        # bounding box location\n",
    "        pred_boxes1=pred_ctr_x-c_to_c_w # Nx1\n",
    "        pred_boxes2=pred_ctr_y-c_to_c_h # Nx1\n",
    "        pred_boxes3=pred_ctr_x+c_to_c_w # Nx1\n",
    "        pred_boxes4=pred_ctr_y+c_to_c_h # Nx1\n",
    "        pred_boxes=torch.hstack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4)) # Nx4\n",
    "        return pred_boxes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13d6037d-b46b-4272-b113-518b028d189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirpath='D:/data/mask_rcnn'\n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "anchors=torch.load(os.path.join(data_dirpath, \"anchors.pt\"),map_location=device, weights_only=True)\n",
    "pred_bbox_deltas=torch.load(os.path.join(data_dirpath, \"pred_bbox_deltas.pt\"),map_location=device, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1a702b8-f16d-4dfa-879a-76c16ecdbac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([370920, 1, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes.reshape(box_sum, -1, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec005689-4787-4b96-9c29-deac56d7d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors_per_images  [185460, 185460]  concat_anchors  torch.Size([370920, 4])  anchors_sum  370920\n",
      "pred_deltas  torch.Size([370920, 4])\n",
      "dx  torch.Size([370920, 1]) -1.0294348001480103 0.7720302939414978\n",
      "dy  torch.Size([370920, 1]) -1.2709095478057861 1.5505026578903198\n",
      "dw  torch.Size([370920, 1]) -2.542602777481079 1.6185859441757202\n",
      "dh  torch.Size([370920, 1]) -2.5830938816070557 2.1535117626190186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([370920, 1, 4]),\n",
       " tensor([[-326.2812, -294.4279,   -2.9163,    1.6288]], grad_fn=<MinBackward0>),\n",
       " tensor([[ 797.8859,  931.3315,  973.0844, 1283.7443]], grad_fn=<MaxBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_coder = BoxCoder(inverse_weights=(1.0, 1.0, 1.0, 1.0))\n",
    "pred_boxes=box_coder.decode(pred_deltas=pred_bbox_deltas, anchors=anchors)\n",
    "pred_boxes.shape, pred_boxes.min(0).values, pred_boxes.max(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad2f42f-d9a0-4e59-8845-474b90ff27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel_codes  torch.Size([370920, 4])\n",
      "boxes  2 [torch.Size([185460, 4]), torch.Size([185460, 4])]\n",
      "boxes_per_images  [185460, 185460]  concat_boxes  torch.Size([370920, 4])  box_sum  370920\n",
      "rel_codes  torch.Size([370920, 4])\n"
     ]
    }
   ],
   "source": [
    "#proposals = box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "rel_codes=pred_bbox_deltas.detach()\n",
    "boxes=anchors # anchors per image\n",
    "print('rel_codes ', rel_codes.shape)\n",
    "print('boxes ', len(boxes), [b.shape for b in boxes])\n",
    "\n",
    "assert isinstance(boxes, (list, tuple)), 'This function expects boxes of type list or tuple'\n",
    "assert isinstance(rel_codes, torch.Tensor), 'This function expects rel_Codes of type torch.Tensor'\n",
    "boxes_per_images=[b.size(0) for b in boxes]\n",
    "concat_boxes=torch.cat(boxes, dim=0)\n",
    "box_sum=sum(boxes_per_images)\n",
    "print('boxes_per_images ', boxes_per_images, ' concat_boxes ', concat_boxes.shape, ' box_sum ', box_sum)\n",
    "if box_sum>0: rel_codes=rel_codes.reshape(box_sum, -1)\n",
    "print('rel_codes ', rel_codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0884e510-96b1-4c81-8b92-aad883f5b2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx  torch.Size([370920, 1]) -1.0294348001480103 0.7720302939414978\n",
      "dy  torch.Size([370920, 1]) -1.2709095478057861 1.5505026578903198\n",
      "dw  torch.Size([370920, 1]) -2.542602777481079 1.6185859441757202\n",
      "dh  torch.Size([370920, 1]) -2.5830938816070557 2.1535117626190186\n"
     ]
    }
   ],
   "source": [
    "# decode_single  pred_boxes = self.decode_single(rel_codes, concat_boxes)\n",
    "# decode_single(self, rel_codes: Tensor, boxes: Tensor) -> Tensor:\n",
    "# From a set of original boxes and encoded relative box offsets, get the decoded boxes\n",
    "# rel_codes --encoded boxes\n",
    "# concat_boxes -- reference boxes\n",
    "concat_boxes=concat_boxes.type(rel_codes.dtype)\n",
    "# elements in each row of refence boxes are x1,y1,x2,y2 where x1<=x2 and y1<=y2 \n",
    "# we compute the width and height of refence boxes\n",
    "width=concat_boxes[:, 2]-concat_boxes[:, 0]  # 1D tensor of size N\n",
    "height=concat_boxes[:, 3]-concat_boxes[:, 1]  # 1D tensor of size N\n",
    "# then we compute the center of the reference boxes\n",
    "ctr_x=concat_boxes[:, 0]+ 0.5*width  # 1D tensor of size N, this is the center\n",
    "ctr_y=concat_boxes[:, 1]+ 0.5*height  # 1D tensor of size N, this is the center\n",
    "\n",
    "wx, wy, ww, wh=box_coder.weights\n",
    "# Nx1 we weights the delta estimates down\n",
    "# dx here is delta scale relative to width for offset along x (e.g., offset from center along x)\n",
    "dx=rel_codes[:, 0::4]/wx # similar to rel_codes[:, 0].unsqueeze(-1) for x/wx\n",
    "# dy here is delta scale relative to height for offset along y (e.g., offset from center along y)\n",
    "dy=rel_codes[:, 1::4]/wy # similar to rel_codes[:, 1].unsqueeze(-1) for y/wy\n",
    "# dw is scale in log-space of width (i.e., apply exp before scaling width)\n",
    "dw=rel_codes[:, 2::4]/ww # similar to rel_codes[:, 2].unsqueeze(-1) for w/ww\n",
    "# dh is scale in log-space of height (i.e., apply exp before scaling height)\n",
    "dh=rel_codes[:, 3::4]/wh # similar to rel_codes[:, 3].unsqueeze(-1) for h/wh\n",
    "print('dx ', dx.shape, dx.min().item(), dx.max().item())\n",
    "print('dy ', dy.shape, dy.min().item(), dy.max().item())\n",
    "print('dw ', dw.shape, dw.min().item(), dw.max().item())\n",
    "print('dh ', dh.shape, dh.min().item(), dh.max().item())\n",
    "\n",
    "# prevent sending too large values to torch.exp\n",
    "dw=torch.clamp(dw, max=box_coder.bbox_xform_clip)\n",
    "dh=torch.clamp(dh, max=box_coder.bbox_xform_clip)\n",
    "\n",
    "# Nx1 = Nx1 Nx1 Nx1\n",
    "pred_ctr_x=dx*width[:,None]+ctr_x[:,None]\n",
    "pred_ctr_y=dy*height[:,None]+ctr_y[:,None]\n",
    "pred_w=torch.exp(dw)*width[:,None]\n",
    "pred_h=torch.exp(dh)*height[:,None]\n",
    "\n",
    "# Nx1 distance from boundary to the box's center. We use these to determine the bounding box\n",
    "# corner location, x1,y1,x2,y2\n",
    "c_to_c_h = torch.tensor(0.5, dtype=pred_ctr_y.dtype, device=pred_h.device)*pred_h\n",
    "c_to_c_w = torch.tensor(0.5, dtype=pred_ctr_x.dtype, device=pred_w.device)*pred_w\n",
    "\n",
    "# bounding box location\n",
    "pred_boxes1=pred_ctr_x-c_to_c_w # Nx1\n",
    "pred_boxes2=pred_ctr_y-c_to_c_h # Nx1\n",
    "pred_boxes3=pred_ctr_x+c_to_c_w # Nx1\n",
    "pred_boxes4=pred_ctr_y+c_to_c_h # Nx1\n",
    "pred_boxes=torch.hstack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4)) # Nx4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0de29b5-51d3-4a89-a8ca-97ed9eed1fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([370920, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16dabd0b-558d-4689-b5bf-06c4fa1f4f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([370920, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df957f30-9a3f-47da-92be-411ff2dd0baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.391249812950831e+27"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_cv",
   "language": "python",
   "name": "op_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
