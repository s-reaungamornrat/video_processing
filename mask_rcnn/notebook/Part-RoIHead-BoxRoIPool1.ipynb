{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a552c202-fdb5-41ee-954d-f1a96886e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import boxes as box_ops\n",
    "from torchvision.ops.boxes import box_area\n",
    "from torchvision.ops.roi_align import _bilinear_interpolate\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c5dd3f-9241-4444-8d0b-e0a64643b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device ', device)\n",
    "\n",
    "main_dirpath='D:/data/'\n",
    "tensor_dirpath=os.path.join(main_dirpath, 'mask_rcnn')\n",
    "original_inputs=torch.load(os.path.join(tensor_dirpath, 'part-backbonefpn-orig_input.pt'), map_location=device, weights_only=True)\n",
    "tfm_inputs=torch.load(os.path.join(tensor_dirpath, 'part-backbonefpn-transform.pt'), map_location=device, weights_only=False)\n",
    "proposal_boxes=torch.load(os.path.join(tensor_dirpath, 'part-rpn-proposal_boxes.pt'), map_location=device, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f120190-6ff9-40e1-874c-afc42245f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of inpute features for classifiers  1024\n",
      "the number of input features for mask  256\n"
     ]
    }
   ],
   "source": [
    "num_classes=2\n",
    "# load an instance segmentation model pre-trained on COCO\n",
    "model=torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='DEFAULT', rpn_pre_nms_top_n_train=800,\n",
    "        rpn_pre_nms_top_n_test=500,  rpn_post_nms_top_n_train=800,  rpn_post_nms_top_n_test=500, box_detections_per_img=100,\n",
    "        box_batch_size_per_image =400, rpn_batch_size_per_image=100)\n",
    "# get number of input features for the classifier\n",
    "in_features=model.roi_heads.box_predictor.cls_score.in_features\n",
    "print('the number of inpute features for classifiers ', in_features)\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor=FastRCNNPredictor(in_features,  num_classes)\n",
    "\n",
    "# get the number of input features for the mask classifiers\n",
    "in_features_mask=model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "print('the number of input features for mask ', in_features_mask)\n",
    "hidden_layer=256\n",
    "# and replace the mask predictor with a new one\n",
    "model.roi_heads.mask_predictor=MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ca670-0057-407a-9b7b-7e932e549b54",
   "metadata": {},
   "source": [
    "[`RoIHeads`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/roi_heads.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509659ad-ba89-401e-b0fe-e05fff62384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box_roi_pool\n",
      "box_head\n",
      "box_predictor\n",
      "mask_roi_pool\n",
      "mask_head\n",
      "mask_predictor\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.roi_heads.named_children(): print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11790d4a-b01d-4af6-8e68-5b13259b72df",
   "metadata": {},
   "source": [
    "[`RoIHeads.forward`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/roi_heads.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1a2c67-d735-4499-9b73-aea0d578cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features  {'0': (torch.Size([2, 256, 208, 264]), -2.1181082725524902, 2.168463945388794), '1': (torch.Size([2, 256, 104, 132]), -1.646379828453064, 1.6166932582855225), '2': (torch.Size([2, 256, 52, 66]), -1.5383321046829224, 1.5961196422576904), '3': (torch.Size([2, 256, 26, 33]), -1.5852569341659546, 1.9735815525054932), 'pool': (torch.Size([2, 256, 13, 17]), -1.4886037111282349, 1.6521222591400146)}\n",
      "proposals  [(torch.Size([800, 4]), 0.0, 1053.0), (torch.Size([800, 4]), 0.0, 830.0)]\n"
     ]
    }
   ],
   "source": [
    "features=tfm_inputs['out']\n",
    "images=tfm_inputs['tfm_images']\n",
    "targets=tfm_inputs['tmf_targets']\n",
    "proposals=proposal_boxes\n",
    "print('features ', {k:(v.shape, v.min().item(), v.max().item()) for k, v in features.items()})\n",
    "print('proposals ', [(p.shape, p.min().item(), p.max().item()) for p in proposals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1513578b-ea33-456b-bb48-75f67e9a46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check targets\n",
    "for t in targets:\n",
    "    if t['boxes'].dtype not in [torch.float, torch.half, torch.double]: raise TypeError(f'target boxes must be float, instead got {t[\"boxes\"].dtype}')\n",
    "    if t['labels'].dtype != torch.int64: raise TypeError(f'target labels must be int64, instead got {t[\"labels\"].dtype}')\n",
    "    if model.roi_heads.has_keypoint():\n",
    "        if t['keypoints'].dtype!=torch.float32: raise TypeError(f'target keypoints must be float, instead got {t[\"keypoints\"].dtype}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31087559-600d-4748-93b1-670a5945fce2",
   "metadata": {},
   "source": [
    "`proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)`\n",
    "\n",
    "[`select_training_samples`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/roi_heads.py#L633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a12527a-ebc1-44dd-92cc-b8c1d8a7b777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposals  [(torch.Size([402, 4]), False), (torch.Size([401, 4]), False)]\n",
      "regression_targets  [torch.Size([400, 4]), torch.Size([400, 4])]\n",
      "proposals  [torch.Size([400, 4]), torch.Size([400, 4])]\n",
      "matched_idxs  [torch.Size([400]), torch.Size([400])]\n",
      "labels  [torch.Size([400]), torch.Size([400])]\n",
      "0 --------------------------------------------------\n",
      "ref  torch.Size([400, 4]) -36731.43359375 521.9777221679688\n",
      "obs  torch.Size([400, 4]) -36731.43359375 521.9777221679688\n",
      "False\n",
      "1 --------------------------------------------------\n",
      "ref  torch.Size([400, 4]) -221.0601043701172 1277.6134033203125\n",
      "obs  torch.Size([400, 4]) -221.0601043701172 1277.6134033203125\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def check_targets(targets, use_mask=False):\n",
    "    '''\n",
    "    targets (list[dict[str, Tensor]])\n",
    "    '''\n",
    "    assert targets is not None, 'targets should not be None'\n",
    "    if not all('boxes' in t for t in targets): raise ValueError('Every element of targets should have a boxes key')\n",
    "    if not all('labels' in t for t in targets): raise ValueError('Every element of targets should have a labels key')\n",
    "    if use_mask:\n",
    "        if not all('masks' in t for t in targets): raise ValueError('Every element of targets should have a masks key')\n",
    "\n",
    "def assign_targets_to_proposals(proposals, gt_boxes,gt_labels, matcher):\n",
    "    '''\n",
    "    Args:\n",
    "        proposals (list[Tensor]): Mx4 boxes/proposals per image\n",
    "        gt_boxes (list[Tensor]): Gx4 ground truth boxes per image\n",
    "        gt_labels (list[Tensor]): G ground truth labels per image\n",
    "        matcher (torchvision.models.detection._utils.Matcher)\n",
    "    Returns:\n",
    "        matched_idxs (list[Tensor]): M indices into corresponding grough truth boxes for each proposal\n",
    "        labels (list[Tensor]): M ground truth labels for each proposal\n",
    "    '''\n",
    "    \n",
    "    matched_idxs, labels=[],[]\n",
    "    for proposals_in_image, gt_boxes_in_image, gt_labels_in_image in zip(proposals, gt_boxes, gt_labels):\n",
    "        if gt_boxes_in_image.numel()==0: # background image\n",
    "            device=proposals_in_image.device\n",
    "            clamped_matched_idx_in_image=torch.zeros((proposals_in_image.shape[0],), dtype=torch.int64, device=device)\n",
    "            labels_in_image=torch.zeros((proposals_in_image.shape[0],), dtype=torch.int64, device=device)\n",
    "        else:\n",
    "            match_quality_matrix=box_ops.box_iou(gt_boxes_in_image, proposals_in_image)\n",
    "            matched_idxs_in_image=matcher(match_quality_matrix) # M indices\n",
    "\n",
    "            clamped_matched_idx_in_image=matched_idxs_in_image.clamp(min=0)\n",
    "            labels_in_image=gt_labels_in_image[clamped_matched_idx_in_image] # index into  G index-tensor by M indices -> M labels\n",
    "            labels_in_image=labels_in_image.to(dtype=torch.int64)\n",
    "            # label background (below low threshold)\n",
    "            bg_inds=matched_idxs_in_image==matcher.BELOW_LOW_THRESHOLD\n",
    "            labels_in_image[bg_inds]=0\n",
    "            # label ignore proposals (between low and high thresholds)\n",
    "            ignore_inds=matched_idxs_in_image==matcher.BETWEEN_THRESHOLDS\n",
    "            labels_in_image[ignore_inds]=-1 # -1 is ignored by sampler\n",
    "        matched_idxs.append(clamped_matched_idx_in_image)\n",
    "        labels.append(labels_in_image)\n",
    "    return matched_idxs, labels\n",
    "\n",
    "def subsample(fg_bg_sampler, labels):\n",
    "    '''\n",
    "    Args:\n",
    "        fg_bg_sampler (torchvision.models.detection._utils.BalancedPositiveNegativeSampler)\n",
    "        labels (list[Tensor]): labels of ground truth for each proposals\n",
    "    '''\n",
    "    sampled_pos_inds, sampled_neg_inds=fg_bg_sampler(labels)\n",
    "    sampled_inds=[]\n",
    "    for img_idx, (pos_inds_img, neg_inds_img) in enumerate(zip(sampled_pos_inds, sampled_neg_inds)):\n",
    "        img_sampled_inds=torch.nonzero(torch.bitwise_or(pos_inds_img, neg_inds_img), as_tuple=True)[0]\n",
    "        # above is similar to torch.where(pos_inds_img | neg_inds_img)[0]\n",
    "        sampled_inds.append(img_sampled_inds)\n",
    "    return sampled_inds\n",
    "    \n",
    "def select_training_samples(use_mask, matcher, fg_bg_sampler, box_coder, proposals, targets):\n",
    "    '''\n",
    "    Args:\n",
    "        proposals (list[Tensor]): Mx4 boxes/proposals per image\n",
    "        targets (list[dict[str, Tensor]]): target per image, each is dict of annotation including Gx4 target boxes in each image\n",
    "    Returns:\n",
    "        proposals (list[Tensor]): Sx4 filtered proposals (balanced positive and negative) per image, where S<=M\n",
    "        matched_idxs (list[Tensor]): S indices to ground-truth for each proposal\n",
    "        labels (list[Tensor]): S ground-truth labels for each proposal \n",
    "        regression_targets (list[Tensor]): Sx4 regression targets (delta/adjustment between proposal and ground-truth)\n",
    "    '''\n",
    "    check_targets(targets=targets, use_mask=use_mask)\n",
    "    dtype, device=proposals[0].dtype, proposals[0].device\n",
    "\n",
    "    gt_boxes=[t['boxes'] for t in targets] # list of Mx4 where M is the number of boxes in each image\n",
    "    gt_labels=[t['labels'] for t in targets] # list of M labels (i.e., 1D tensor) each for each box in each image\n",
    "    \n",
    "    # append ground-truth bboxes to propos\n",
    "    #proposals = self.add_gt_proposals(proposals, gt_boxes)\n",
    "    proposals=[torch.cat([proposal, gt_box], dim=0) for proposal, gt_box in zip(proposals, gt_boxes)]# list of Mx4 boxes per image\n",
    "    print('proposals ', [(p.shape, p.requires_grad) for p in proposals])\n",
    "\n",
    "    # get matching gt indices for each proposal\n",
    "    # matched_idxs, labels = self.assign_targets_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "    # matched_idxs are indices into gt_boxes to get ground-truth boxes for each proposal\n",
    "    # labels are ground truth labels for each ground-truth associated with proposals\n",
    "    matched_idxs, labels=assign_targets_to_proposals(proposals, gt_boxes,gt_labels, matcher=matcher)\n",
    "    # matched_idxs_ref, labels_ref = model.roi_heads.assign_targets_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "    # print('matched_idxs ', [torch.allclose(m, n) for m, n in zip(matched_idxs, matched_idxs_ref)])\n",
    "    # print('labels ', [torch.allclose(m, n) for m, n in zip(labels, labels_ref)])\n",
    "\n",
    "    # sample a fixed porportion of positive-negative proposals\n",
    "    #sampled_inds = self.subsample(labels)\n",
    "    sampled_inds=subsample(fg_bg_sampler, labels)\n",
    "\n",
    "    matched_gt_boxes=[]\n",
    "    num_images=len(proposals)\n",
    "    for img_id in range(num_images):\n",
    "        img_sampled_inds=sampled_inds[img_id]\n",
    "        proposals[img_id]=proposals[img_id][img_sampled_inds]\n",
    "        labels[img_id]=labels[img_id][img_sampled_inds]\n",
    "        matched_idxs[img_id]=matched_idxs[img_id][img_sampled_inds]\n",
    "\n",
    "        gt_boxes_in_image=gt_boxes[img_id]\n",
    "        if gt_boxes_in_image.numel()==0:\n",
    "            gt_boxes_in_image=torch.zeros((1,4), dtype=dtype, device=device)\n",
    "        matched_gt_boxes.append(gt_boxes_in_image[matched_idxs[img_id]])\n",
    "\n",
    "    regression_targets=box_coder.encode(matched_gt_boxes, proposals)\n",
    "    print('regression_targets ',[r.shape for r in  regression_targets])\n",
    "    print('proposals ',[p.shape for p in proposals])\n",
    "    print('matched_idxs ', [m.shape for m in matched_idxs])\n",
    "    print('labels ', [l.shape for l in labels])\n",
    "    return proposals, matched_idxs, labels, regression_targets\n",
    "\n",
    "\n",
    "            \n",
    "proposals, matched_idxs, labels, regression_targets=select_training_samples(use_mask=model.roi_heads.has_mask(), matcher=model.roi_heads.proposal_matcher, \n",
    "                        fg_bg_sampler=model.roi_heads.fg_bg_sampler,box_coder=model.roi_heads.box_coder,\n",
    "                        proposals=proposals, targets=targets)\n",
    "\n",
    "proposals_ref, matched_idxs_ref, labels_ref, regression_targets_ref = model.roi_heads.select_training_samples(proposals, targets)\n",
    "for i, (ref, obs) in enumerate(zip(regression_targets_ref, regression_targets)):\n",
    "    print(i, '-'*50)\n",
    "    print('ref ', ref.shape, ref.min().item(), ref.max().item())\n",
    "    print('obs ', obs.shape, obs.min().item(), obs.max().item())\n",
    "    print(torch.allclose(ref, obs)) # internally call torch.randperm so it is not deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7b7f6-451f-411c-a9da-cf22d8aa0f54",
   "metadata": {},
   "source": [
    "[`MultiScaleRoIAlign`](https://github.com/pytorch/vision/blob/main/torchvision/ops/poolers.py)\n",
    "\n",
    "```\n",
    "box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58c010a3-8da6-4c0e-8acf-0e398616fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n",
      "feature names  odict_keys(['0', '1', '2', '3', 'pool'])\n"
     ]
    }
   ],
   "source": [
    "print(model.roi_heads.box_roi_pool.featmap_names)\n",
    "print('feature names ', features.keys())\n",
    "\n",
    "def _filter_input(x: dict[str, Tensor], featmap_names: list[str])->list[Tensor]:\n",
    "    '''\n",
    "    Only select features whose names are in `featmap_names\n",
    "    '''\n",
    "    x_filtered=[]\n",
    "    for k, v in x.items():\n",
    "        if k in featmap_names: x_filtered.append(v)\n",
    "    return x_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "64c8a909-9951-4085-ab64-e294c26d2572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_filtered  [torch.Size([2, 256, 208, 264]), torch.Size([2, 256, 104, 132]), torch.Size([2, 256, 52, 66]), torch.Size([2, 256, 26, 33])]\n"
     ]
    }
   ],
   "source": [
    "x=features\n",
    "# x_filtered=_filter_input(x, self.featmap_names)\n",
    "x_filtered=_filter_input(x, model.roi_heads.box_roi_pool.featmap_names)\n",
    "print('x_filtered ', [i.shape for i in x_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ab9c8-0dbe-4796-a73d-8441ecf35d74",
   "metadata": {},
   "source": [
    "[_setup_scales](https://github.com/pytorch/vision/blob/main/torchvision/ops/poolers.py#L110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d862d83d-5f1b-4d9e-a910-f8ac459a4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_scale(feature: Tensor, original_size: list[int])->float:\n",
    "    # assume the scale is of the form 2**(-k) where k is integer\n",
    "    size=feature.shape[-2:]\n",
    "    possible_scales:list[float]=[]\n",
    "    for s1, s2 in zip(size, original_size):\n",
    "        approx_scale=float(s1)/float(s2)\n",
    "        scale=2**float(torch.tensor(approx_scale).log2().round())\n",
    "        possible_scales.append(scale)\n",
    "        break # we note that x and y direction returns the same scale so we just compute on x direction only\n",
    "    return possible_scales[0]\n",
    "    \n",
    "class LevelMapper: # see https://github.com/pytorch/vision/blob/main/torchvision/ops/poolers.py#L47\n",
    "    '''\n",
    "    Determine which FPN level each RoI should map to based on the heuristic in the FPN paper\n",
    "    Args:\n",
    "        k_min (int): the 1st level of FPN considered (shallow level with finer feature)\n",
    "        k_max (int): the last level of FPN (deeper level with coarser feature)\n",
    "        canonical_scale (int): geometric size of input (size used to pretrain backbone, e.g., 224)\n",
    "        canonical_level (int): base FPN level (e.g., 4)\n",
    "        eps (float)\n",
    "    '''\n",
    "    def __init__(self, k_min:int, k_max: int, canonical_scale:int=224, canonical_level:int=4, eps:float=1.e-6):\n",
    "        self.k_max=k_max\n",
    "        self.k_min=k_min\n",
    "        self.s0=canonical_scale\n",
    "        self.lvl0=canonical_level\n",
    "        self.eps=eps\n",
    "    def __call__(self, boxlists: list[Tensor])->Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            boxlists (list[Tensor])\n",
    "        '''\n",
    "        # compute level ids\n",
    "        s=torch.sqrt(torch.cat([box_area(boxlist) for boxlist in boxlists])) # M geomtric mean size\n",
    "        print(f'In LevelMapper s {s.shape}')\n",
    "\n",
    "        # eq.1 in FPN paper\n",
    "        target_lvls=torch.floor(self.lvl0+torch.log2(s/self.s0)+torch.tensor(self.eps, dtype=s.dtype))\n",
    "        target_lvls=torch.clamp(target_lvls, min=self.k_min, max=self.k_max)\n",
    "        return (target_lvls.to(torch.int64)-self.k_min).to(torch.int64)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65bb03bf-a9ee-41d8-8033-e950c4b0d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.roi_heads.box_roi_pool.scales  [0.25, 0.125, 0.0625, 0.03125]\n"
     ]
    }
   ],
   "source": [
    "# if self.scales is None or self.map_levels is None:\n",
    "#             self.scales, self.map_levels = _setup_scales(\n",
    "#                 x_filtered, image_shapes, self.canonical_scale, self.canonical_level\n",
    "#             )\n",
    "\n",
    "def _setup_scales(features: list[Tensor], image_shapes: list[tuple[int, int]], canonical_scale:int, \n",
    "                  canonical_level:int, eps:float=1e-6)->tuple[list[float], LevelMapper]:\n",
    "    \n",
    "    assert image_shapes is not None, 'image list should not be empty'\n",
    "\n",
    "    max_x=max_y=0\n",
    "    for shape in image_shapes:\n",
    "        max_x=max(shape[0], max_x)\n",
    "        max_y=max(shape[1], max_y)\n",
    "    original_input_shape=(max_x, max_y)\n",
    "\n",
    "    scales=[]\n",
    "    for feat in features:\n",
    "        scale=_infer_scale(feat, original_input_shape)\n",
    "        scales.append(scale)\n",
    "\n",
    "    # get the levels in the feature map by leveraging the fact that the network always downsamples by\n",
    "    # a factor of 2 at each level\n",
    "    lvl_min=-np.log2(scales[0])\n",
    "    lvl_max=-np.log2(scales[-1])\n",
    "\n",
    "    map_levels=LevelMapper(k_min=int(lvl_min), k_max=int(lvl_max), canonical_scale=canonical_scale,\n",
    "                           canonical_level=canonical_level, eps=eps)\n",
    "    \n",
    "    \n",
    "    return scales, map_levels\n",
    "\n",
    "    \n",
    "model.roi_heads.box_roi_pool.scales, model.roi_heads.box_roi_pool.map_levels  = _setup_scales(\\\n",
    "                x_filtered, images.image_sizes, model.roi_heads.box_roi_pool.canonical_scale,\n",
    "                model.roi_heads.box_roi_pool.canonical_level)\n",
    "print('model.roi_heads.box_roi_pool.scales ', model.roi_heads.box_roi_pool.scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cb175-ad65-41c3-8d8e-eddf59eb111b",
   "metadata": {},
   "source": [
    "```\n",
    "_multiscale_roi_align(\n",
    "            x_filtered,\n",
    "            boxes,\n",
    "            self.output_size,\n",
    "            self.sampling_ratio,\n",
    "            self.scales,\n",
    "            self.map_levels,\n",
    "        )\n",
    "```\n",
    "\n",
    "[`_multiscale_roi_align`](https://github.com/pytorch/vision/blob/main/torchvision/ops/poolers.py#L147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2cd9110f-b4db-4e86-9636-1e0f7b01d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_align(input, boxes, output_size, spatial_scale, sampling_ratio, aligned=False):\n",
    "    '''\n",
    "    Perform region of interest (ROI) align operator with average pooling, as described in Mask R-CNN\n",
    "    Args:\n",
    "        input (Tensor[N,C,H,W]): the input tensor\n",
    "        boxes (Tensor[K,5] or List[Tensor[L,4]]): the box coordinates in (x1,y1,x2,y2) where the regions will be traken from\n",
    "            If a single tensor is passed, the first column should be the index of the corresponding element in the batch, i.e., a number in ``[0, N-1]``\n",
    "            If a list of Tensors, the each Tensor will correspond to the boxes for an element i in the batch\n",
    "        output_size (int or Tuple[int, int]): the size of output (in bins or pixels) after the pooling is performed as (height width)\n",
    "        spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the \n",
    "            scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), y'll want to set\n",
    "            this to 0.5\n",
    "        sampling_ratio (int): number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. \n",
    "            if >0, exactly ``sampling_ratioxsampling_ratio`` sampling points per bin are used. If <=0, an adaptive number of grid points are used \n",
    "            (computed as ``ceil(roi_width/output_width)``, and likewise for height)\n",
    "        aligned (bool): If False, use legacy implementation. Otherwise, pixel shift the box coordinates by -.5 for a better alignment with the two\n",
    "            neighboring pixel indices. This version used in Detectron2\n",
    "    Returns:\n",
    "        Tensor[K,C,output_size[0], output_size[1]]: the pooled ROIs\n",
    "    '''\n",
    "    #_roi_align(input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned)\n",
    "    pooled_height, pooled_width=output_size[0], output_size[1]\n",
    "    orig_dtype=input.dtype\n",
    "    rois=boxes # Kx5\n",
    "    # input NxCxHxW\n",
    "    height, width=input.shape[2:]\n",
    "    ph=torch.arange(pooled_height, device=input.device) # [PH]\n",
    "    pw=torch.arange(pooled_width, device=input.device) # [PW]\n",
    "\n",
    "    roi_batch_ind=rois[:, 0].int() # [K]\n",
    "    offset=0.5 if aligned else 0.\n",
    "    roi_start_w=rois[:,1]*spatial_scale-offset #[K]\n",
    "    roi_start_h=rois[:,2]*spatial_scale-offset #[K]\n",
    "    roi_end_w=rois[:,3]*spatial_scale-offset #[K]\n",
    "    roi_end_h=rois[:,4]*spatial_scale-offset #[K]\n",
    "\n",
    "    roi_width=roi_end_w-roi_start_w # [K]\n",
    "    roi_height=roi_end_h-roi_start_h # [K]\n",
    "    if not aligned:\n",
    "        roi_width=torch.clamp(roi_width, min=1.) # [K]\n",
    "        roi_height=torch.clamp(roi_height, min=1.) # [K]  \n",
    "\n",
    "    bin_size_h=roi_height/pooled_height # [K]\n",
    "    bin_size_w=roi_width/pooled_width # [K]\n",
    "    exact_sampling=sampling_ratio>0\n",
    "    roi_bin_grid_h=sampling_ratio if exact_sampling else torch.ceil(roi_height/pooled_height) # scalar or [K]\n",
    "    roi_bin_grid_w=sampling_ratio if exact_sampling else torch.ceil(roi_width/pooled_width) # scalar or [K]\n",
    "\n",
    "    if exact_sampling:\n",
    "        count=max(roi_bin_grid_h*roi_bin_grid_w, 1) # scalar\n",
    "        iy=torch.arange(roi_bin_grid_h, device=input.device) # [IY]\n",
    "        ix=torch.arange(roi_bin_grid_w, device=input.device) # [IX]\n",
    "        ymask=xmask=None\n",
    "    else:\n",
    "        count=torch.clamp(roi_bin_grid_h*roi_bin_grid_w, min=1) # [K]\n",
    "        # when doing adaptive sampling, the number of samples we need to do is data-dependent based on how big ROIs are. This is a bit awkward\n",
    "        # because first class dims cannot actually handle this. So instead, we inefficiently suppose that we needed to sample all the points and \n",
    "        # mask out things that turned out to be unnecessary\n",
    "        iy=torch.arange(height, device=input.device) # [IY]\n",
    "        ix=torch.arange(width, device=input.device) # [IX]\n",
    "        ymask=iy[None,:]<roi_bin_grid_h[:,None] # [K, IY]\n",
    "        xmask=ix[None,:]<roi_bin_grid_w[:,None] # [K, IX]\n",
    "    def from_K(t): return t[:,None,None]\n",
    " \n",
    "        # [K,1,1]              #[1 PH 1] [K,1,1]                   # [1,1,IY][K,1,1]\n",
    "    y=(from_K(roi_start_h)+   ph[None,:,None]*from_K(bin_size_h)+ (iy[None,None,:]+0.5).to(input.dtype)*from_K(bin_size_h/roi_bin_grid_h))#[K,PH,IY]\n",
    "    x=(from_K(roi_start_w)+ pw[None,:,None]*from_K(bin_size_w)+ (ix[None, None,:]+.5).to(input.dtype)*from_K(bin_size_w/roi_bin_grid_w)) # [K PW IX]\n",
    "    print('roi_batch_ind ', roi_batch_ind.shape, roi_batch_ind.unique())\n",
    "    val=_bilinear_interpolate(input, roi_batch_ind, y, x, ymask, xmask) # [K,C,PH, PW, IY, IX]\n",
    "\n",
    "    # mask out samples that weren't actually adpatively needed\n",
    "    if not exact_sampling:\n",
    "        val = torch.where(ymask[:, None, None, None, :, None], val, 0)\n",
    "        val = torch.where(xmask[:, None, None, None, None, :], val, 0)\n",
    "\n",
    "    output=val.sum((-1, -2)) # remove IY, IX -> [K, C, PH, PW]\n",
    "    if isinstance(count, torch.Tensor): output/=count[:,None,None,None]\n",
    "    else:output/=count\n",
    "    output=output.to(dtype=orig_dtype)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fd675f63-e0bf-4006-a3f7-04dc6ae6de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LevelMapper s torch.Size([800])\n",
      "result  torch.Size([800, 256, 7, 7])\n",
      "0 torch.Size([2, 256, 208, 264]) 0.25\n",
      "rois_per_level  torch.Size([453, 5])\n",
      "roi_batch_ind  torch.Size([453]) tensor([0, 1], dtype=torch.int32)\n",
      "result_idx_in_level  torch.Size([453, 256, 7, 7])\n",
      "1 torch.Size([2, 256, 104, 132]) 0.125\n",
      "rois_per_level  torch.Size([154, 5])\n",
      "roi_batch_ind  torch.Size([154]) tensor([0, 1], dtype=torch.int32)\n",
      "result_idx_in_level  torch.Size([154, 256, 7, 7])\n",
      "2 torch.Size([2, 256, 52, 66]) 0.0625\n",
      "rois_per_level  torch.Size([159, 5])\n",
      "roi_batch_ind  torch.Size([159]) tensor([0, 1], dtype=torch.int32)\n",
      "result_idx_in_level  torch.Size([159, 256, 7, 7])\n",
      "3 torch.Size([2, 256, 26, 33]) 0.03125\n",
      "rois_per_level  torch.Size([34, 5])\n",
      "roi_batch_ind  torch.Size([34]) tensor([0, 1], dtype=torch.int32)\n",
      "result_idx_in_level  torch.Size([34, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "def _multiscale_roi_align(x_filtered: list[Tensor], boxes:list[Tensor], \n",
    "output_size:list[int], sampling_ratio:int, scales:Optional[list[float]],\n",
    "mapper:Optional[LevelMapper])->Tensor:\n",
    "    '''\n",
    "    Args:\n",
    "        x_filtered (list[Tensor]): list of features from backbone FPN\n",
    "        boxes (list[Tensor[N,4]]): boxes to be used to perform the pooling operation, in (x1, y1, x2, y2) format\n",
    "            and in the image reference size, not the feature map reference. The coordinate must satisfy \n",
    "            ``0<=x1<x2`` and ``0<=y1<y2``.\n",
    "        output_size (Union[list[tuple[int, int]], list[int]]): size of output\n",
    "        sampling_ratio (int): sampling ratio for RoIAlign\n",
    "        scales (Optional[list[float]]): the ratio between feature size and image size\n",
    "        mapper (Optional[LevelMapper]): \n",
    "    Returns:\n",
    "        result (Tensor):\n",
    "    '''\n",
    "    assert all(x is not None for x in [scales, mapper]), 'scales and mapper should not be None'\n",
    "    num_levels=len(x_filtered)\n",
    "\n",
    "    # concatenate proposals to form a single tensor and append its with image id so we know which image\n",
    "    # which proposal associated with\n",
    "    #rois = _convert_to_roi_format(boxes)\n",
    "    concat_boxes=torch.cat(boxes, dim=0) # Mx4\n",
    "    device, dtype=concat_boxes.device, concat_boxes.dtype\n",
    "    # Mx1\n",
    "    ids=torch.cat([torch.full_like(b[:,:1], i, dtype=dtype, device=device) for i, b in enumerate(boxes)], dim=0)\n",
    "    # Mx5\n",
    "    rois=torch.cat([ids, concat_boxes], dim=1)\n",
    "\n",
    "    levels=mapper(boxes) # M levels for each box \n",
    "\n",
    "    num_rois=len(rois)\n",
    "    num_channels=x_filtered[0].shape[1]\n",
    "    dtype, device=x_filtered[0].dtype, x_filtered[0].device\n",
    "    result=torch.zeros((num_rois, num_channels)+output_size, dtype=dtype, device=device)\n",
    "    print('result ', result.shape)\n",
    "    for level, (per_level_feature, scale) in enumerate(zip(x_filtered, scales)):\n",
    "        # per_level_feature is of size BxCxHxW where B is batch size, C is the number of channel\n",
    "        print(level, per_level_feature.shape, scale)\n",
    "        idx_in_level=torch.nonzero(levels==level, as_tuple=True)[0]\n",
    "        rois_per_level=rois[idx_in_level]\n",
    "\n",
    "        print('rois_per_level ', rois_per_level.shape)\n",
    "        #  result_idx_in_level = roi_align(per_level_feature, rois_per_level, output_size=output_size, spatial_scale=scale, sampling_ratio=sampling_ratio,)\n",
    "        result_idx_in_level = roi_align(per_level_feature, rois_per_level, output_size=output_size, spatial_scale=scale,\n",
    "                                        sampling_ratio=sampling_ratio,)\n",
    "        print('result_idx_in_level ', result_idx_in_level.shape)\n",
    "        result[idx_in_level]=result_idx_in_level.to(dtype=result.dtype)\n",
    "    return result\n",
    "    \n",
    "result=_multiscale_roi_align(x_filtered, boxes=proposals, \n",
    "                      output_size=model.roi_heads.box_roi_pool.output_size, \n",
    "                      sampling_ratio=model.roi_heads.box_roi_pool.sampling_ratio, \n",
    "                      scales=model.roi_heads.box_roi_pool.scales,\n",
    "                      mapper=model.roi_heads.box_roi_pool.map_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "949eb88d-819f-449c-b4ce-1f645b15b426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(453+154+159+34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cd0f4826-12c9-4286-98f4-bafc3c60d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'box_features':result,\n",
    "           'proposals':proposals, \n",
    "            'matched_idxs':matched_idxs, 'labels':labels, 'regression_targets':regression_targets},\n",
    "           os.path.join(tensor_dirpath, 'part-roi_head-box_roi_pool.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "40289698-31f6-46de-8e0a-98c47b4e7e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result  torch.Size([800, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print('result ', result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
