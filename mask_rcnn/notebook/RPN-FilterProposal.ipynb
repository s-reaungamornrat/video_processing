{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd58480-b3e8-4f87-9eda-5a13f2df0d43",
   "metadata": {},
   "source": [
    "This check the function [`filter_proposals`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/rpn.py) in `RPN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cacc8b4-1264-4594-b0bd-7d52ed02509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.ops import boxes as box_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a56962-e4fe-43ed-8ee5-b7e6163ca23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposals  torch.Size([2, 211038, 4]) tensor(-430.7388) tensor(1432.4897)\n",
      "objectness  torch.Size([422076, 1]) tensor(-23.0735, grad_fn=<MinBackward1>) tensor(9.0714, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "data_dirpath='D:/data/mask_rcnn'\n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "objectness=torch.load(os.path.join(data_dirpath, \"objectness.pt\"),map_location=device, weights_only=True) # NHWAxC\n",
    "proposals=torch.load(os.path.join(data_dirpath, \"proposals.pt\"),map_location=device, weights_only=True) #NxHWAx4C\n",
    "print('proposals ', proposals.shape, proposals.min(), proposals.max())\n",
    "print('objectness ', objectness.shape, objectness.min(), objectness.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843377f9-d97c-4313-b661-da1a317a3030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectness  torch.Size([2, 211038])\n"
     ]
    }
   ],
   "source": [
    "image_shapes=[(800, 1033), (800, 1026)]\n",
    "num_anchors_per_level=[np.int64(158400), np.int64(39600), np.int64(9900), np.int64(2475), np.int64(663)]\n",
    "\n",
    "num_images=proposals.shape[0] # batch size\n",
    "device=proposals.device\n",
    "\n",
    "# do not backprop through objectness\n",
    "objectness=objectness.detach()\n",
    "objectness=objectness.reshape(num_images, -1) # from NHWAxC to NxHWAC=NxHWA where C=1 typically\n",
    "print('objectness ', objectness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1eb866-3eb3-45df-8972-95a142879bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _topk_min(input, orig_kval, axis):\n",
    "    \"\"\"\n",
    "    Given the original k-value (orig_kval), check whether the k-value <= the number of input values. \n",
    "    If not, change return the number of input values instead of original k-values\n",
    "    Args:\n",
    "        input (Tensor): The original input tensor.\n",
    "        orig_kval (int): The provided k-value.\n",
    "        axis(int): Axis along which we retrieve the input size.\n",
    "    Returns:\n",
    "        min_kval (int): Appropriately selected k-value.\n",
    "    \"\"\"\n",
    "    return min(orig_kval, input.size(axis))\n",
    "\n",
    "# pre_nms_top_n={'training':2000, 'testing':1000}\n",
    "def get_top_n_idx(objectness, num_anchors_per_level, pre_nms_top_n=2000):\n",
    "    '''\n",
    "    Get the indices of the top n highest objectness values\n",
    "    Args:\n",
    "        objectness (tensor): NxHWA from all levels\n",
    "        num_anchors_per_level (sequence): the list of number of bounding boxes per level\n",
    "        pre_nms_top_n (int): the number of top k objectness before NMS\n",
    "    Return:\n",
    "        top_k (tensor): NxM where M=Sum(min(K, HWA)) for each level. If, for all levels, HWA>K, then Nx(LK) where L is the number of levels\n",
    "    '''\n",
    "    # select top_n boxes independently per level before applying NMS\n",
    "    r,offset=[],0\n",
    "    for ob in objectness.split(num_anchors_per_level, 1): # split objectness into the number per level\n",
    "        num_anchors=ob.shape[1] # NxHWA\n",
    "        #pre_nms_top_n = det_utils._topk_min(ob, self.pre_nms_top_n(), 1)\n",
    "        pre_nms_top_n=_topk_min(input=ob,orig_kval=pre_nms_top_n,axis=1)\n",
    "        _,top_n_idx=ob.topk(pre_nms_top_n, dim=1) # NxK\n",
    "        r.append(top_n_idx+offset)\n",
    "        offset+=num_anchors\n",
    "    return torch.cat(r, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e156cf5-0d0d-40ac-aef3-59685e793dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levels  [torch.Size([158400]), torch.Size([39600]), torch.Size([9900]), torch.Size([2475]), torch.Size([663])]\n",
      "levels  torch.Size([2, 211038])\n",
      "top_n_idx  tensor([[ 37959,   3966,   3174,  ..., 210480, 210429, 210431],\n",
      "        [ 69578,  68786,  69576,  ..., 210430, 210379, 210421]])\n"
     ]
    }
   ],
   "source": [
    "levels=[torch.full((n,), idx, dtype=torch.int64, device=device) for idx, n in enumerate(num_anchors_per_level)]\n",
    "print('levels ', [l.shape for l in levels])\n",
    "levels=torch.cat(levels, dim=0)\n",
    "levels=levels.reshape(1, -1).expand_as(objectness)\n",
    "print('levels ', levels.shape)\n",
    "\n",
    "# select top_n boxes independently per level before applying nms\n",
    "#top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)\n",
    "top_n_idx=get_top_n_idx(objectness, num_anchors_per_level, pre_nms_top_n=2000)\n",
    "print('top_n_idx ', top_n_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c18c47-8d9c-40ea-8f69-d1c508cb419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_range  tensor([0, 1])  batch_idx  tensor([[0],\n",
      "        [1]])\n",
      "objectness  torch.Size([2, 8663])  levels  torch.Size([2, 8663])  proposals  torch.Size([2, 8663, 4])\n",
      "objectness  tensor(-15.5430) tensor(9.0714)\n",
      "objectness_prob  tensor(1.7773e-07) tensor(0.9999)\n"
     ]
    }
   ],
   "source": [
    "image_range=torch.arange(num_images, device=device) # where num_images is batch size\n",
    "batch_idx=image_range[:,None] # Nx1\n",
    "print('image_range ', image_range, ' batch_idx ', batch_idx)\n",
    "\n",
    "# select top k objectness\n",
    "objectness=objectness[batch_idx, top_n_idx] # NxK \n",
    "levels=levels[batch_idx, top_n_idx] # NxK \n",
    "proposals=proposals[batch_idx, top_n_idx] # NxK \n",
    "objectness_prob=torch.sigmoid(objectness)\n",
    "print('objectness ', objectness.shape, ' levels ', levels.shape, ' proposals ', proposals.shape)\n",
    "print('objectness ', objectness.min(), objectness.max())\n",
    "print('objectness_prob ', objectness_prob.min(), objectness_prob.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c73767-5dbc-44f8-94a3-75ec163bea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --------------------------------------------------\n",
      "\tboxes torch.Size([8663, 4]), scores torch.Size([8663]), lvl torch.Size([8663])\n",
      "\tboxes torch.Size([8663, 4]), scores torch.Size([8663]), lvl torch.Size([8663])\n",
      "\tboxes torch.Size([8663, 4]), scores torch.Size([8663]), lvl torch.Size([8663])\n",
      "\tnms_thresh 0.7, scores torch.Size([8663]), (1.7773150773336965e-07, 0.999885082244873)\n",
      "\tnms_thresh 0.7, scores torch.Size([2000]), (0.10210991650819778, 0.999885082244873)\n",
      "1 --------------------------------------------------\n",
      "\tboxes torch.Size([8663, 4]), scores torch.Size([8663]), lvl torch.Size([8663])\n",
      "\tboxes torch.Size([8658, 4]), scores torch.Size([8658]), lvl torch.Size([8658])\n",
      "\tboxes torch.Size([8658, 4]), scores torch.Size([8658]), lvl torch.Size([8658])\n",
      "\tnms_thresh 0.7, scores torch.Size([8658]), (0.00010085690882988274, 0.9996401071548462)\n",
      "\tnms_thresh 0.7, scores torch.Size([2000]), (0.0072363014332950115, 0.9996401071548462)\n",
      "\n",
      "final_boxes  [torch.Size([2000, 4]), torch.Size([2000, 4])]\n",
      "\n",
      "final_scores  [torch.Size([2000]), torch.Size([2000])]\n"
     ]
    }
   ],
   "source": [
    "min_size=0.001\n",
    "score_thresh=0.0  \n",
    "nms_thresh=0.7\n",
    "post_nms_top_n=2000\n",
    "final_boxes=[]\n",
    "final_scores=[]\n",
    "for i, (boxes, scores, lvl, img_shape) in enumerate(zip(proposals, objectness_prob, levels, image_shapes)):\n",
    "    print(i, '-'*50)\n",
    "    print(f'\\tboxes {boxes.shape}, scores {scores.shape}, lvl {lvl.shape}')\n",
    "    # boxes Kx4 where K from all levels, and the 4 are for x1,y1,x2,y2 \n",
    "    boxes = box_ops.clip_boxes_to_image(boxes, img_shape) # img_shape is HxW or YxX\n",
    "    # remove small boxes\n",
    "    keep_idx=box_ops.remove_small_boxes(boxes, min_size)\n",
    "    boxes,scores,lvl=boxes[keep_idx],scores[keep_idx],lvl[keep_idx]\n",
    "    print(f'\\tboxes {boxes.shape}, scores {scores.shape}, lvl {lvl.shape}')\n",
    "    # remove low scoring boxes\n",
    "    # use >= for backward compatibility\n",
    "    keep_idx=torch.where(scores>=score_thresh)[0]\n",
    "    boxes,scores,lvl=boxes[keep_idx],scores[keep_idx],lvl[keep_idx]\n",
    "    print(f'\\tboxes {boxes.shape}, scores {scores.shape}, lvl {lvl.shape}')\n",
    "    print(f'\\tnms_thresh {nms_thresh}, scores {scores.shape}, ({scores.min()}, {scores.max()})')\n",
    "\n",
    "    # non-maximum suppression, independently done per level\n",
    "    keep_idx= box_ops.batched_nms(boxes, scores, lvl, nms_thresh) # HWAx4\n",
    "\n",
    "    # keep only topk scoring prediction\n",
    "    keep_idx=keep_idx[:post_nms_top_n]\n",
    "    boxes,scores=boxes[keep_idx],scores[keep_idx]\n",
    "    print(f'\\tnms_thresh {nms_thresh}, scores {scores.shape}, ({scores.min()}, {scores.max()})')\n",
    "\n",
    "    final_boxes.append(boxes)\n",
    "    final_scores.append(scores)\n",
    "\n",
    "print('\\nfinal_boxes ', [x.shape for x in final_boxes])\n",
    "print('\\nfinal_scores ', [x.shape for x in final_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9d12c0-62ec-49b5-9581-76513faf05be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --------------------------------------------------\n",
      "\tboxes  torch.Size([2000, 4]) tensor([0.0000, 0.0000, 2.8989, 4.0253]) tensor([1019.4376,  797.7085, 1033.0000,  800.0000])\n",
      "\tscores  torch.Size([2000]) tensor(0.1021) tensor(0.9999)\n",
      "1 --------------------------------------------------\n",
      "\tboxes  torch.Size([2000, 4]) tensor([0.0000, 0.0000, 2.5135, 3.6417]) tensor([1023.5251,  797.5223, 1026.0000,  800.0000])\n",
      "\tscores  torch.Size([2000]) tensor(0.0072) tensor(0.9996)\n"
     ]
    }
   ],
   "source": [
    "for b, (boxes, scores) in enumerate(zip(final_boxes, final_scores)):\n",
    "    print(b, '-'*50)\n",
    "    print('\\tboxes ', boxes.shape, boxes.min(0).values, boxes.max(0).values)\n",
    "    print('\\tscores ', scores.shape, scores.min(), scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b7610-7a36-46aa-ac7d-ba5a799998ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445aec2e-7fbe-4da8-a742-96bdbb0c5714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_cv",
   "language": "python",
   "name": "op_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
