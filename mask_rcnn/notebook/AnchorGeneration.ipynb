{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a3474b-e519-484f-ba7b-644f38d58ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9ec3d-bef1-4b46-900f-00a4b15a1e91",
   "metadata": {},
   "source": [
    "### Anchor Bounding Box\n",
    "\n",
    "This is the [`generate_anchors`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/anchor_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e92672-fb47-4b22-b4cd-0280d8d8d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios=((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69a6310-6141-48d0-88d3-386b7b337981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "5 \n",
      " [torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4])] \n",
      " [tensor([[-22.6274, -11.3137,  22.6274,  11.3137],\n",
      "        [-16.0000, -16.0000,  16.0000,  16.0000],\n",
      "        [-11.3137, -22.6274,  11.3137,  22.6274]]), tensor([[-45.2548, -22.6274,  45.2548,  22.6274],\n",
      "        [-32.0000, -32.0000,  32.0000,  32.0000],\n",
      "        [-22.6274, -45.2548,  22.6274,  45.2548]]), tensor([[-90.5097, -45.2548,  90.5097,  45.2548],\n",
      "        [-64.0000, -64.0000,  64.0000,  64.0000],\n",
      "        [-45.2548, -90.5097,  45.2548,  90.5097]]), tensor([[-181.0193,  -90.5097,  181.0193,   90.5097],\n",
      "        [-128.0000, -128.0000,  128.0000,  128.0000],\n",
      "        [ -90.5097, -181.0193,   90.5097,  181.0193]]), tensor([[-362.0387, -181.0193,  362.0387,  181.0193],\n",
      "        [-256.0000, -256.0000,  256.0000,  256.0000],\n",
      "        [-181.0193, -362.0387,  181.0193,  362.0387]])]\n"
     ]
    }
   ],
   "source": [
    "# for every (size, aspect_ratio) combination, output a zero-centered anchor\n",
    "# We assume aspect_ratio=height/width \n",
    "\n",
    "device=torch.device('cpu')\n",
    "dtype=torch.float32\n",
    "\n",
    "cell_anchors=[]\n",
    "for size, aspect_ratio in zip(sizes, aspect_ratios):\n",
    "    size=torch.as_tensor(size, dtype=dtype, device=device) # 1D say of size S\n",
    "    aspect_ratio=torch.as_tensor(aspect_ratio, dtype=dtype, device=device) #1D say of size A\n",
    "    # why sqrt?\n",
    "    h_ratio=torch.sqrt(aspect_ratio) # sqrt(height/width)\n",
    "    w_ratio=1./h_ratio # sqrt(width/height)\n",
    "    print('h_ratio ', h_ratio)\n",
    "    print('w_ratio ', w_ratio)\n",
    "    # Ax1 1xS -> AxS -> AS\n",
    "    ws=(w_ratio[:,None]*size[None,:]).view(-1)\n",
    "    hs=(h_ratio[:,None]*size[None,:]).view(-1)\n",
    "\n",
    "    base_anchors=torch.stack([-ws, -hs, ws, hs], dim=1)/2\n",
    "    cell_anchors.append(base_anchors)\n",
    "print(len(cell_anchors), '\\n', [a.shape for a in cell_anchors], '\\n', cell_anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d81bc-a7a9-400e-865c-6792a3a29ffd",
   "metadata": {},
   "source": [
    "### Compute anchors on grid\n",
    "\n",
    "This is the [`grid_anchors`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/anchor_utils.py) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d49ad7-6d6a-4970-b4f7-15ab95002313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strides  [[tensor(4), tensor(4)], [tensor(8), tensor(8)], [tensor(16), tensor(16)], [tensor(32), tensor(32)], [tensor(61), tensor(61)]]\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')\n",
    "grid_sizes=[torch.Size([200, 232]), torch.Size([100, 116]), torch.Size([50, 58]), torch.Size([25, 29]), torch.Size([13, 15])] \n",
    "image_size=torch.Size([800, 928])\n",
    "strides=[\n",
    "    [torch.empty((), dtype=torch.int64, device=device).fill_(image_size[0]//g[0]),\n",
    "     torch.empty((), dtype=torch.int64, device=device).fill_(image_size[1]//g[1])]\n",
    "    for g in grid_sizes\n",
    "]\n",
    "print('strides ', strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525d9d73-6a99-4d7d-a629-1c7bfd5f3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shifts_x  torch.Size([232]) 0 924 torch.int32\n",
      "shifts_y  torch.Size([200]) 0 796 torch.int32\n",
      "shift_x  torch.Size([200, 232]) 0 924 torch.int32 tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36], dtype=torch.int32)\n",
      "shift_y  torch.Size([200, 232]) 0 796 torch.int32 tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36], dtype=torch.int32)\n",
      "shifts  torch.Size([46400, 4]) tensor([0, 0, 0, 0], dtype=torch.int32) tensor([924, 796, 924, 796], dtype=torch.int32) torch.int32\n",
      "shifts.view(shifts.shape[0], 1, shifts.shape[-1])  torch.Size([46400, 1, 4])\n",
      "base_anchors.view(1,*base_anchors.shape)  torch.Size([1, 3, 4])\n",
      "anchors  [torch.Size([139200, 4])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "anchors=[]\n",
    "assert cell_anchors is not None, 'cell_anchors should not be None'\n",
    "assert len(grid_sizes)==len(strides)==len(cell_anchors), 'Anchors should be Tuple[Tuple[int]] '\\\n",
    "'because each feature map could potentially have different sizes and aspect ratios. There needs to be a match between the number of'\\\n",
    "'feature maps passed and the number of sizes and aspect ratios specified'\n",
    "\n",
    "for size, stride, base_anchors in zip(grid_sizes, strides, cell_anchors):\n",
    "    grid_height, grid_width=size\n",
    "    stride_height, stride_width=stride\n",
    "    device=base_anchors.device\n",
    "    \n",
    "    # For output anchor, compute [x_center, y_center, x_center, y_center]\n",
    "    # We associate each grid location/index (gx,gy) to location on image via stride\n",
    "    shifts_x=torch.arange(0, grid_width, dtype=torch.int32, device=device)*stride_width # location on image im_x = stride_x * gx\n",
    "    shifts_y=torch.arange(0, grid_height, dtype=torch.int32, device=device)*stride_height # location on image im_y = stride_y * gy\n",
    "    print('shifts_x ', shifts_x.shape, shifts_x.min().item(), shifts_x.max().item(), shifts_x.dtype)\n",
    "    print('shifts_y ', shifts_y.shape, shifts_y.min().item(), shifts_y.max().item(), shifts_y.dtype)\n",
    "    # Let H=gy and W=gx. Below shift_y and shift_x are of size HxW\n",
    "    shift_y, shift_x=torch.meshgrid(shifts_y, shifts_x, indexing='ij')\n",
    "    print('shift_x ', shift_x.shape, shift_x.min().item(), shift_x.max().item(), shift_x.dtype, shift_x[0][:10])\n",
    "    print('shift_y ', shift_y.shape, shift_y.min().item(), shift_y.max().item(), shift_y.dtype, shift_y[:,0][:10])\n",
    "    shift_x=shift_x.reshape(-1) # HW\n",
    "    shift_y=shift_y.reshape(-1) # HW\n",
    "    # each linked feature index/location corresponds to location on image\n",
    "    shifts=torch.stack([shift_x, shift_y, shift_x, shift_y], dim=1) # HWx4 where each row is x,y,x,y\n",
    "    print('shifts ', shifts.shape, shifts.min(dim=0).values, shifts.max(dim=0).values, shifts.dtype)\n",
    "\n",
    "    # For every (base_anchor, output anchor) pair, offset each zero-centered base anchor by the center of the output anchor\n",
    "    n_coords=base_anchors.shape[-1] # 4 for 2D, i.e., x1,y1,x2,y2\n",
    "    # base_anchors is of size Ax4 where A is the number of anchors, which is equal to the number of aspect ratio\n",
    "    #              ( HWx1x4                                           1xAx4 ) -> HWxAx4 -> HWAx4\n",
    "    print('shifts.view(shifts.shape[0], 1, shifts.shape[-1]) ',\n",
    "         shifts.view(shifts.shape[0], 1, shifts.shape[-1]).shape)\n",
    "    print('base_anchors.view(1,*base_anchors.shape) ', base_anchors.view(1,*base_anchors.shape).shape)\n",
    "    anchors.append((shifts.view(shifts.shape[0], 1, shifts.shape[-1])+base_anchors.view(1,*base_anchors.shape)).view(-1, n_coords))\n",
    "    break\n",
    "print('anchors ', [anchor.shape for anchor in anchors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e716540e-1846-47b3-af15-99afbb5c33c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200*232*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ead0c55-e9f6-453f-99e2-d104ddad67e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_sizes=[torch.Size([200, 232]), torch.Size([100, 116]), torch.Size([50, 58]), torch.Size([25, 29]), torch.Size([13, 15])] \n",
    "image_size=torch.Size([800, 928])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_cv",
   "language": "python",
   "name": "op_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
