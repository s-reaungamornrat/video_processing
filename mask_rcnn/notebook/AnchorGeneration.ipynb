{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a3474b-e519-484f-ba7b-644f38d58ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9ec3d-bef1-4b46-900f-00a4b15a1e91",
   "metadata": {},
   "source": [
    "### Anchor Bounding Box\n",
    "\n",
    "This is the [`generate_anchors`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/anchor_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e92672-fb47-4b22-b4cd-0280d8d8d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios=((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69a6310-6141-48d0-88d3-386b7b337981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "h_ratio  tensor([0.7071, 1.0000, 1.4142])\n",
      "w_ratio  tensor([1.4142, 1.0000, 0.7071])\n",
      "5 \n",
      " [torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4])] \n",
      " [tensor([[-22.6274, -11.3137,  22.6274,  11.3137],\n",
      "        [-16.0000, -16.0000,  16.0000,  16.0000],\n",
      "        [-11.3137, -22.6274,  11.3137,  22.6274]]), tensor([[-45.2548, -22.6274,  45.2548,  22.6274],\n",
      "        [-32.0000, -32.0000,  32.0000,  32.0000],\n",
      "        [-22.6274, -45.2548,  22.6274,  45.2548]]), tensor([[-90.5097, -45.2548,  90.5097,  45.2548],\n",
      "        [-64.0000, -64.0000,  64.0000,  64.0000],\n",
      "        [-45.2548, -90.5097,  45.2548,  90.5097]]), tensor([[-181.0193,  -90.5097,  181.0193,   90.5097],\n",
      "        [-128.0000, -128.0000,  128.0000,  128.0000],\n",
      "        [ -90.5097, -181.0193,   90.5097,  181.0193]]), tensor([[-362.0387, -181.0193,  362.0387,  181.0193],\n",
      "        [-256.0000, -256.0000,  256.0000,  256.0000],\n",
      "        [-181.0193, -362.0387,  181.0193,  362.0387]])]\n"
     ]
    }
   ],
   "source": [
    "# for every (size, aspect_ratio) combination, output a zero-centered anchor\n",
    "# We assume aspect_ratio=height/width \n",
    "\n",
    "device=torch.device('cpu')\n",
    "dtype=torch.float32\n",
    "\n",
    "cell_anchors=[]\n",
    "for size, aspect_ratio in zip(sizes, aspect_ratios):\n",
    "    size=torch.as_tensor(size, dtype=dtype, device=device) # 1D say of size S\n",
    "    aspect_ratio=torch.as_tensor(aspect_ratio, dtype=dtype, device=device) #1D say of size A\n",
    "    # why sqrt?\n",
    "    h_ratio=torch.sqrt(aspect_ratio) # sqrt(height/width)\n",
    "    w_ratio=1./h_ratio # sqrt(width/height)\n",
    "    print('h_ratio ', h_ratio)\n",
    "    print('w_ratio ', w_ratio)\n",
    "    # Ax1 1xS -> AxS -> AS\n",
    "    ws=(w_ratio[:,None]*size[None,:]).view(-1)\n",
    "    hs=(h_ratio[:,None]*size[None,:]).view(-1)\n",
    "\n",
    "    base_anchors=torch.stack([-ws, -hs, ws, hs], dim=1)/2\n",
    "    cell_anchors.append(base_anchors)\n",
    "print(len(cell_anchors), '\\n', [a.shape for a in cell_anchors], '\\n', cell_anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d81bc-a7a9-400e-865c-6792a3a29ffd",
   "metadata": {},
   "source": [
    "### Compute anchors on grid\n",
    "\n",
    "This is the [`grid_anchors`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/anchor_utils.py) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "525d9d73-6a99-4d7d-a629-1c7bfd5f3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strides  [[tensor(4), tensor(4)], [tensor(8), tensor(8)], [tensor(16), tensor(16)], [tensor(32), tensor(32)], [tensor(61), tensor(62)]]\n",
      "\tshifts_x  torch.Size([328]) 0 1308 torch.int32\n",
      "shifts_y  torch.Size([200]) 0 796 torch.int32\n",
      "shift_x  torch.Size([200, 328]) 0 1308 torch.int32 tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36], dtype=torch.int32)\n",
      "shift_y  torch.Size([200, 328]) 0 796 torch.int32 tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36], dtype=torch.int32)\n",
      "shifts  torch.Size([65600, 4]) tensor([0, 0, 0, 0], dtype=torch.int32) tensor([1308,  796, 1308,  796], dtype=torch.int32) torch.int32\n",
      "anchors  [torch.Size([65600, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')\n",
    "grid_sizes=[torch.Size([200, 328]), torch.Size([100, 164]), torch.Size([50, 82]), torch.Size([25, 41]), torch.Size([13, 21])] \n",
    "image_size=torch.Size([800, 1312])\n",
    "strides=[\n",
    "    [torch.empty((), dtype=torch.int64, device=device).fill_(image_size[0]//g[0]),\n",
    "     torch.empty((), dtype=torch.int64, device=device).fill_(image_size[1]//g[1])]\n",
    "    for g in grid_sizes\n",
    "]\n",
    "print('strides ', strides)\n",
    "\n",
    "anchors=[]\n",
    "assert cell_anchors is not None, 'cell_anchors should not be None'\n",
    "assert len(grid_sizes)==len(strides)==len(cell_anchors), 'Anchors should be Tuple[Tuple[int]] '\\\n",
    "'because each feature map could potentially have different sizes and aspect ratios. There needs to be a match between the number of'\\\n",
    "'feature maps passed and the number of sizes and aspect ratios specified'\n",
    "\n",
    "for size, stride, base_anchors in zip(grid_sizes, strides, cell_anchors):\n",
    "    grid_height, grid_width=size\n",
    "    stride_height, stride_width=stride\n",
    "    device=base_anchors.device\n",
    "    \n",
    "    # For output anchor, compute [x_center, y_center, x_center, y_center]\n",
    "    # We associate each grid location/index (gx,gy) to location on image via stride\n",
    "    shifts_x=torch.arange(0, grid_width, dtype=torch.int32, device=device)*stride_width # location on image im_x = stride_x * gx\n",
    "    shifts_y=torch.arange(0, grid_height, dtype=torch.int32, device=device)*stride_height # location on image im_y = stride_y * gy\n",
    "    print('\\tshifts_x ', shifts_x.shape, shifts_x.min().item(), shifts_x.max().item(), shifts_x.dtype)\n",
    "    print('shifts_y ', shifts_y.shape, shifts_y.min().item(), shifts_y.max().item(), shifts_y.dtype)\n",
    "    shift_y, shift_x=torch.meshgrid(shifts_y, shifts_x, indexing='ij')\n",
    "    print('shift_x ', shift_x.shape, shift_x.min().item(), shift_x.max().item(), shift_x.dtype, shift_x[0][:10])\n",
    "    print('shift_y ', shift_y.shape, shift_y.min().item(), shift_y.max().item(), shift_y.dtype, shift_y[:,0][:10])\n",
    "    shift_x=shift_x.reshape(-1)\n",
    "    shift_y=shift_y.reshape(-1)\n",
    "    # each linked feature index/location corresponds to location on image\n",
    "    shifts=torch.stack([shift_x, shift_y, shift_x, shift_y], dim=1) # WHWH or XYXY\n",
    "    print('shifts ', shifts.shape, shifts.min(dim=0).values, shifts.max(dim=0).values, shifts.dtype)\n",
    "\n",
    "    # For every (base_anchor, output anchor) pair, offset each zero-centered base anchor by the center of the output anchor\n",
    "    n_coords=base_anchors.shape[-1] # 4 for 2D, i.e., x1,y1,x2,y2\n",
    "    # Let n=gxgy, nx1x4 + 1xnbx4 -> nxnbx4 -> (n*nb)x4\n",
    "    anchors.append(shifts.view(shifts.shape[0], 1, shifts.shape[-1])+base_anchors.view(1,*base_anchors.shape).view(-1, n_coords))\n",
    "    break\n",
    "print('anchors ', [anchor.shape for anchor in anchors])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_cv",
   "language": "python",
   "name": "op_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
