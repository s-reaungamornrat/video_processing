{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035771ff-ebb4-49e2-a2eb-c1220ca9fae8",
   "metadata": {
    "id": "035771ff-ebb4-49e2-a2eb-c1220ca9fae8"
   },
   "source": [
    "Replacing `https://github.com` by `https://colab.research.google.com/github` as mentioned in [stackoverflow](https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce02a7-0239-4829-9ba6-bd059a306b1d",
   "metadata": {
    "id": "22ce02a7-0239-4829-9ba6-bd059a306b1d"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9b27c-0265-44cd-be9a-a24cb2e3f900",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0a9b27c-0265-44cd-be9a-a24cb2e3f900",
    "outputId": "88f80192-da55-4d88-a34e-1f407abd7e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-23 08:38:34--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
      "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
      "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 53723336 (51M) [application/zip]\n",
      "Saving to: ‘data/PennFudanPed.zip’\n",
      "\n",
      "PennFudanPed.zip    100%[===================>]  51.23M  33.2MB/s    in 1.5s    \n",
      "\n",
      "2025-09-23 08:38:36 (33.2 MB/s) - ‘data/PennFudanPed.zip’ saved [53723336/53723336]\n",
      "\n",
      "/content\n",
      "Cloning into 'video_processing'...\n",
      "remote: Enumerating objects: 382, done.\u001b[K\n",
      "remote: Counting objects: 100% (382/382), done.\u001b[K\n"
     ]
    }
   ],
   "source": [
    "# if run in Google colab\n",
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
    "!pwd\n",
    "!cd data && unzip PennFudanPed.zip > load_data.log\n",
    "\n",
    "!git clone https://github.com/s-reaungamornrat/video_processing.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8df31-c283-4673-8cdb-305b6dbcac7e",
   "metadata": {
    "id": "e3b8df31-c283-4673-8cdb-305b6dbcac7e"
   },
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d935b0e-bac3-4013-bb75-c0cfdde89b84",
   "metadata": {
    "id": "2d935b0e-bac3-4013-bb75-c0cfdde89b84"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed=0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb53ac9-526f-44c8-813f-e63bc072415e",
   "metadata": {
    "id": "ccb53ac9-526f-44c8-813f-e63bc072415e"
   },
   "outputs": [],
   "source": [
    "from video_processing.yolov7.parameter_parser import parser\n",
    "from video_processing.yolov7.models.model import Model\n",
    "from video_processing.yolov7.models.ema import ModelEMA\n",
    "from video_processing.yolov7.loss.module import ComputeLoss\n",
    "from video_processing.yolov7.train.utils import setup_optimizer, labels_to_class_weights, train_an_epoch\n",
    "from video_processing.yolov7.dataset.penn_fandu_dataset import PennFudanDataset\n",
    "from video_processing.yolov7.utils.general import one_cycle, check_image_size\n",
    "from video_processing.yolov7.dataset.anchors import check_anchor_matching\n",
    "from video_processing.yolov7.test.utils import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tpa-sRoCcF4F",
   "metadata": {
    "id": "Tpa-sRoCcF4F"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab7ee3-0f10-44bf-aa7d-fd5a6cb7b204",
   "metadata": {
    "id": "b6ab7ee3-0f10-44bf-aa7d-fd5a6cb7b204"
   },
   "outputs": [],
   "source": [
    "data_dirpath='data/PennFudanPed'\n",
    "result_dirpath='results'\n",
    "\n",
    "argument=f\"\"\"\n",
    "--data-dirpath {data_dirpath} --output-dirpath {result_dirpath}\n",
    "--worker 1 --device cuda --batch-size 2 --nominal-batch-size 2\n",
    "--img 1280 1280 --cfg yolov7-w6-pennfandu.yaml\n",
    "--weights ''  --name yolov7-w6 --hyp hyp.scratch.p6.yaml --correct-exif --print-freq 40\n",
    "--epochs 100\n",
    "\"\"\"\n",
    "# --n-training-data 100 --n-val-data 20 --dev-mode\n",
    "args=parser.parse_args(argument.split())\n",
    "\n",
    "device=torch.device('cpu' if not torch.cuda.is_available() or args.device=='cpu' else 'cuda')\n",
    "print(f'Computing on {device} using batch size of {args.batch_size}')\n",
    "\n",
    "if not os.path.isdir(args.output_dirpath):os.makedirs(args.output_dirpath)\n",
    "args.checkpoint_dirpath=os.path.join(args.output_dirpath, args.checkpoint_dirname)\n",
    "if not os.path.isdir(args.checkpoint_dirpath): os.makedirs(args.checkpoint_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495a9e4-028f-4bea-a70e-e99990104399",
   "metadata": {
    "id": "2495a9e4-028f-4bea-a70e-e99990104399"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "with open(args.hyp) as f: hyp=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# number of classes\n",
    "nc=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5f2a0-4dee-47d8-bc92-be801e196ebc",
   "metadata": {
    "id": "1bf5f2a0-4dee-47d8-bc92-be801e196ebc"
   },
   "outputs": [],
   "source": [
    "# train/val data loader\n",
    "# Here since the data are small we are going to overfit training\n",
    "train_dataset=PennFudanDataset(root=args.data_dirpath, image_dirname=args.image_dirname, mask_dirname=args.mask_dirname, hyp=hyp,\n",
    "                        img_size=args.img_size[0], augment=False,  indices=np.arange(160))\n",
    "val_dataset=PennFudanDataset(root=args.data_dirpath, image_dirname=args.image_dirname, mask_dirname=args.mask_dirname, hyp=hyp,\n",
    "                        img_size=args.img_size[0], augment=False,  indices=np.arange(160,170))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, num_workers=1, pin_memory=True,\n",
    "                                        collate_fn=PennFudanDataset.collate_fn)\n",
    "val_loader=torch.utils.data.DataLoader(dataset=val_dataset, batch_size=args.batch_size, num_workers=1, pin_memory=True,\n",
    "                                        collate_fn=PennFudanDataset.collate_fn)\n",
    "\n",
    "print(f'There are {len(train_dataset)} training images and {len(val_dataset)} validation images ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a4b6e-ecb2-48c6-8f12-999f0902e8c9",
   "metadata": {
    "id": "516a4b6e-ecb2-48c6-8f12-999f0902e8c9"
   },
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "from video_processing.yolov7.dataset.coords import normalized_xywh2xyxy\n",
    "\n",
    "cmap = plt.get_cmap('tab10', 10)\n",
    "plt.rcParams.update({'font.size'   : 12})\n",
    "\n",
    "imgs, labels, fpath,_=next(iter(train_loader))\n",
    "_, ax=plt.subplots(1,imgs.shape[0],figsize=(6,3))\n",
    "for i, img in enumerate(imgs):\n",
    "    is_box=labels[:,0]==i # box of this image\n",
    "    boxes=labels[:,2:][is_box]\n",
    "    boxes=normalized_xywh2xyxy(boxes,w=img.shape[-1], h=img.shape[1])\n",
    "    ax[i].imshow(img.permute(1,2,0)) # CxHxW to HxWxC\n",
    "    for j, box in enumerate(boxes):\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle(box[:2], box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor=cmap(j), facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax[i].add_patch(rect)\n",
    "plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "\n",
    "imgs, labels, fpath,_=next(iter(val_loader))\n",
    "_, ax=plt.subplots(1,imgs.shape[0],figsize=(6,3))\n",
    "for i, img in enumerate(imgs):\n",
    "    is_box=labels[:,0]==i # box of this image\n",
    "    boxes=labels[:,2:][is_box]\n",
    "    boxes=normalized_xywh2xyxy(boxes,w=img.shape[-1], h=img.shape[1])\n",
    "    ax[i].imshow(img.permute(1,2,0)) # CxHxW to HxWxC\n",
    "    for j, box in enumerate(boxes):\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle(box[:2], box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor=cmap(j), facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax[i].add_patch(rect)\n",
    "plt.tight_layout(pad=0, w_pad=0, h_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d331de-a8ac-435d-900a-e8a2013b5169",
   "metadata": {
    "id": "c1d331de-a8ac-435d-900a-e8a2013b5169"
   },
   "outputs": [],
   "source": [
    "# define model and optimizers\n",
    "model=Model(args.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # it is safer to move model to device first and then create optimizer\n",
    "optimizer=setup_optimizer(model, learning_rate=hyp['lr0'], momentum=hyp['momentum'], weight_decay=hyp['weight_decay'])\n",
    "scheduler=torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=one_cycle(1., hyp['lrf'], args.epochs))\n",
    "model_ema=ModelEMA(model)\n",
    "start_epoch, best_fitness, best_loss=1, -np.inf, np.inf\n",
    "if args.resume is not None and os.path.isfile(os.path.join(args.checkpoint_dirpath, args.resume)):\n",
    "    resume_fpath=os.path.join(args.checkpoint_dirpath, args.resume)\n",
    "    if os.path.isfile(os.path.join(args.checkpoint_dirpath, args.best_checkpoint_fname)):\n",
    "        resume_fpath=os.path.join(args.checkpoint_dirpath, args.best_checkpoint_fname)\n",
    "    checkpoint = torch.load(resume_fpath, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    if 'best_fitness' in checkpoint: best_fitness=checkpoint['best_fitness']\n",
    "    if 'best_loss' in checkpoint: best_loss=checkpoint['best_loss']\n",
    "    model_ema.ema.load_state_dict(checkpoint['ema'])\n",
    "    model_ema.updates=checkpoint['updates']\n",
    "\n",
    "# wrap distributed training\n",
    "\n",
    "# check that the image size divisible by stride\n",
    "max_stride=max(int(model.stride.max()), 32)\n",
    "assert check_image_size(image_size=args.img_size, stride=max_stride), f'{args.img_size} must be divisible by {max_stride}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393ba7b-af92-4025-ad1a-efbe458ccff1",
   "metadata": {
    "id": "9393ba7b-af92-4025-ad1a-efbe458ccff1"
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "nl=model.model[-1].nl\n",
    "print(\"hyp['box'] \", hyp['box'],  \" hyp['cls'] \", hyp['cls'], \" hyp['obj] \", hyp['obj'] )\n",
    "hyp['box']*=3./nl # box-regression loss weight scaled to layer\n",
    "hyp['cls']*= nc/80. * 3./nl # classification loss weight scaled to classes and layers\n",
    "hyp['obj']*=(args.img_size[0]/640)**2 *3./nl # objectness loss weight scaled to image size and layers\n",
    "hyp['label_smoothing']=args.label_smoothing\n",
    "model.nc=nc # attach number of classes to model\n",
    "model.hyp=hyp\n",
    "# blending factor between fixed objectness of 1 and IoU between prediction and ground truth\n",
    "# used to set target objectness, i.e., target_objectness = (1-gr)+gr*iou\n",
    "model.gr=1.\n",
    "print(\"hyp['box'] \", hyp['box'],  \" hyp['cls'] \", hyp['cls'], \" hyp['obj] \", hyp['obj'], ' args.label_smoothing ', args.label_smoothing )\n",
    "scheduler.last_epoch=start_epoch-1 # do not move?\n",
    "train_loss_module=ComputeLoss(model, cls_pw=hyp['cls_pw'], obj_pw=hyp['obj_pw'], label_smoothing=args.label_smoothing, use_aux=True)\n",
    "val_loss_module=ComputeLoss(model, cls_pw=hyp['cls_pw'], obj_pw=hyp['obj_pw'], label_smoothing=args.label_smoothing, use_aux=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b70648-f900-491b-9c28-20c26fe82025",
   "metadata": {
    "id": "a5b70648-f900-491b-9c28-20c26fe82025"
   },
   "outputs": [],
   "source": [
    "train_log = open(args.train_log, 'a')\n",
    "train_log.write('t-w-loss, t-w-bb, t-w-obj, t-w-cls, t-bb, t-obj, t-cls, v-w-loss, v-w-bb, v-w-obj, v-w-cls, v-bb, v-obj, v-cls\\n')\n",
    "train_log.close()\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    # train a model\n",
    "    model.train()\n",
    "    mean_weighted_loss, mean_unweigthed_loss=train_an_epoch(args, model, model_ema, optimizer, train_loss_module, train_loader, epoch=epoch)\n",
    "\n",
    "    # scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # mAP validation model\n",
    "    model_ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride']) #, 'class_weights'])\n",
    "\n",
    "    # validation: since it is small test, we use the model itself\n",
    "    mean_val_weighted_loss,mean_val_unweighted_loss=validate(model=model, dataloader=val_loader, val_loss_module=val_loss_module, hyp=hyp,\n",
    "                                      n_its=1 if args.dev_mode else None)\n",
    "\n",
    "    val_txt=', '.join(f'{n}:{v.item():.3f}' for n, v in zip('bb,obj,cls'.split(','),mean_val_unweighted_loss))\n",
    "    lr_txt=', '.join([f\"{params['lr']:.3f}\"  for params in optimizer.param_groups])\n",
    "    print(f'Epoch {epoch}: train-loss={mean_weighted_loss[0].item():.3f}, val-loss={mean_val_weighted_loss[0].item():.3f}, {val_txt}, lr=({lr_txt})', flush=True)\n",
    "\n",
    "    # save losses for plotting\n",
    "    training_losses.append(mean_weighted_loss[0].item())\n",
    "    val_losses.append(mean_val_weighted_loss[0].item())\n",
    "\n",
    "    # save parameters\n",
    "    training_state_dict={'epoch': epoch, 'model': model.state_dict(), 'ema':model_ema.ema.state_dict(),  'optimizer': optimizer.state_dict(),\n",
    "                         'scheduler': scheduler.state_dict(), 'updates':model_ema.updates, 'best_loss': best_loss}\n",
    "    torch.save(training_state_dict, os.path.join(args.checkpoint_dirpath, args.resume))\n",
    "    if mean_val_weighted_loss[0].item() < best_loss:\n",
    "        best_loss=mean_val_weighted_loss[0].item()\n",
    "        training_state_dict['best_loss']=best_loss\n",
    "        torch.save(training_state_dict, os.path.join(args.checkpoint_dirpath, args.best_checkpoint_fname))\n",
    "\n",
    "    # write log\n",
    "    log=', '.join(f'{x.item()}' for x in mean_weighted_loss) + \\\n",
    "        ', '.join(f'{x.item()}' for x in mean_unweigthed_loss) + \\\n",
    "        ', '.join(f'{x.item()}' for x in mean_val_weighted_loss) + \\\n",
    "        ', '.join(f'{x.item()}' for x in mean_val_unweighted_loss) \n",
    "\n",
    "    train_log = open(args.train_log, 'a')\n",
    "    train_log.write(log+'\\n')\n",
    "    train_log.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
