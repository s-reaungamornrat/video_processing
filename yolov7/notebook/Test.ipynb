{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c95283-c317-4560-a876-b0a6dd772551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21a3e0d1c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed=0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "## slower, more reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = False, True\n",
    "## faster, less reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a42b41e-aee7-49c1-b066-856515a22324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from video_processing.yolov7.parameter_parser import parser\n",
    "from video_processing.yolov7.models.model import Model\n",
    "from video_processing.yolov7.models.ema import ModelEMA\n",
    "from video_processing.yolov7.loss.module import ComputeLoss\n",
    "from video_processing.yolov7.train.utils import setup_optimizer, labels_to_class_weights, train_an_epoch\n",
    "from video_processing.yolov7.dataset.coco_dataset import LoadImagesAndLabels\n",
    "from video_processing.yolov7.utils.general import one_cycle, check_image_size\n",
    "from video_processing.yolov7.dataset.anchors import check_anchor_matching\n",
    "\n",
    "from video_processing.yolov7.metrics.confusion_matrix import ConfusionMatrix\n",
    "from video_processing.yolov7.utils.general import non_max_suppression\n",
    "from video_processing.yolov7.dataset.coords import adjust_coords, xywh2xyxy\n",
    "from video_processing.yolov7.test.utils import statistics_per_image, validation\n",
    "from video_processing.yolov7.metrics.utils import ap_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ff4bad-9e2e-4053-9bf9-c3bbb9c161c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n",
    "    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n",
    "    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n",
    "    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n",
    "    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n",
    "    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n",
    "         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb796d6-ee18-4b82-a5d9-8983e12bf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirpath='D:/data/coco'\n",
    "result_dirpath='D:/results/yolov7'\n",
    "\n",
    "argument=f\"\"\"\n",
    "--data-dirpath {data_dirpath}/coco --output-dirpath {result_dirpath} \n",
    "--worker 1 --device cpu --batch-size 2 --data coco.yaml --img 1280 1280 --cfg yolov7-w6.yaml\n",
    "--weights ''  --name yolov7-w6 --hyp hyp.scratch.p6.yaml \n",
    "--n-training-data 100 --n-val-data 20 --correct-exif --print-freq 1 \n",
    "--dev-mode\n",
    "\"\"\"\n",
    "args=parser.parse_args(argument.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c7e940-6721-4792-916b-a0ac7e299d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 2\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu' if not torch.cuda.is_available() or args.device=='cpu' else 'cuda')\n",
    "print(device, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60bf1482-550f-4a73-bacd-7d5d0cfa79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.is_coco  True\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "with open(args.hyp) as f: hyp=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# data\n",
    "args.is_coco=len(re.findall(\"coco.yaml$\", args.data))>0\n",
    "print('args.is_coco ', args.is_coco)\n",
    "with open(args.data) as f: data_dict=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# number of classes\n",
    "nc=1 if args.single_cls else int(data_dict['nc']) \n",
    "names=['item'] if args.single_cls and len(data_dict['names'])!=1 else data_dict['names'] # class names\n",
    "assert len(names)==nc, f'There are {len(names)} class names but {nc} classes' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8752cb2b-63b3-4bc2-a6a9-6bf091cbab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 3\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n"
     ]
    }
   ],
   "source": [
    "model=Model(args.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # it is safer to move model to device first and then create optimizer\n",
    "model_ema=ModelEMA(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721612ae-cb2f-448c-82d9-232bf1295f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset.coco_dataset.__init__ save cache to D:\\data\\coco\\coco\\labels\\val2017.cache cache_path.is_file() True\n"
     ]
    }
   ],
   "source": [
    "# train/val data loader\n",
    "val_dataset=LoadImagesAndLabels(data_dirpath=args.data_dirpath, image_paths=data_dict['val'], img_size=args.img_size[0],\n",
    "                            augment=False, hyp=hyp, n_data=args.n_val_data, correct_exif=args.correct_exif)\n",
    "val_loader=torch.utils.data.DataLoader(dataset=val_dataset, batch_size=args.batch_size, num_workers=1, pin_memory=True, \n",
    "                                        collate_fn=LoadImagesAndLabels.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87b25b1-74d9-442e-8cc6-2a3f2c0de9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp['box']  0.05  hyp['cls']  0.3  hyp['obj]  0.7\n",
      "hyp['box']  0.037500000000000006  hyp['cls']  0.22499999999999998  hyp['obj]  2.0999999999999996  args.label_smoothing  0.0\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "nl=model.model[-1].nl\n",
    "print(\"hyp['box'] \", hyp['box'],  \" hyp['cls'] \", hyp['cls'], \" hyp['obj] \", hyp['obj'] )\n",
    "hyp['box']*=3./nl # box-regression loss weight scaled to layer\n",
    "hyp['cls']*= nc/80. * 3./nl # classification loss weight scaled to classes and layers\n",
    "hyp['obj']*=(args.img_size[0]/640)**2 *3./nl # objectness loss weight scaled to image size and layers\n",
    "hyp['label_smoothing']=args.label_smoothing\n",
    "model.nc=nc # attach number of classes to model\n",
    "model.hyp=hyp\n",
    "# blending factor between fixed objectness of 1 and IoU between prediction and ground truth\n",
    "# used to set target objectness, i.e., target_objectness = (1-gr)+gr*iou\n",
    "model.gr=1. \n",
    "model.names=data_dict['names']\n",
    "print(\"hyp['box'] \", hyp['box'],  \" hyp['cls'] \", hyp['cls'], \" hyp['obj] \", hyp['obj'], ' args.label_smoothing ', args.label_smoothing )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dae9e47-caff-4659-9408-b87eb5701e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_module=ComputeLoss(model, cls_pw=hyp['cls_pw'], obj_pw=hyp['obj_pw'], label_smoothing=args.label_smoothing, use_aux=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e92b3d-219c-4750-b133-e50963f4d028",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a9486c-8258-47db-a2c6-827499edae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83736545-1378-4143-b4d1-d2eb741314ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all           0           0           0           0           0\n"
     ]
    }
   ],
   "source": [
    "results, maps=validation(model=model_ema.ema, dataloader=val_loader, val_loss_module=val_loss_module, hyp=hyp, \n",
    "                         n_classes=1 if single_cls else int(data_dict['nc']), conf_thres=0.001, iou_thres=0.6, verbose=False,\n",
    "                         n_its=1 if args.dev_mode else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a130afda-fdf0-4db7-9967-f57917dbc0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  tensor([1.3386, 0.0308, 1.4209]),\n",
       "  tensor([0.6374, 0.1371, 1.4209])),\n",
       " (0.0, 0.0, 0.0, 0.0))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, results[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2e9fc1e-d088-437c-9975-fdc151b6cf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.338644027709961"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[4][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9830b79c-1a60-4096-b176-88a0d4692ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc7138a-616b-4523-8536-cf6ebaeff92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97f7bae-34bd-4567-92f7-e1505810f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc  80\n",
      "b_idx  1\n",
      "b_idx  2\n",
      "b_idx  3\n",
      "b_idx  4\n",
      "b_idx  5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52072f79-653a-495c-a7c1-edb282be27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3000, 10), (3000,), (3000,), (67,)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de121b25-20cd-4f2c-8a40-b27236ce550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
