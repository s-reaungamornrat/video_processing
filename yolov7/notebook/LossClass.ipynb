{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c9fb1a-c884-451e-8762-5df7b04af72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bfeb605c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed=0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "## slower, more reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = False, True\n",
    "## faster, less reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcc5aa3-578e-40a3-ae0b-8520deb57c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from video_processing.yolov7.parameter_parser import parser\n",
    "from video_processing.yolov7.models.model import Model\n",
    "from video_processing.yolov7.train.utils import setup_optimizer, labels_to_class_weights\n",
    "from video_processing.yolov7.dataset.coco_dataset import LoadImagesAndLabels\n",
    "from video_processing.yolov7.utils.general import one_cycle, check_image_size\n",
    "from video_processing.yolov7.dataset.anchors import check_anchor_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585e7804-1a1c-4227-90cc-e75b1e22cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 2\n"
     ]
    }
   ],
   "source": [
    "data_dirpath='D:/data/coco'\n",
    "result_dirpath='D:/results/yolov7'\n",
    "\n",
    "argument=f\"\"\"\n",
    "--data-dirpath {data_dirpath}/coco --output-dirpath {result_dirpath} \n",
    "--worker 1 --device cpu --batch-size 2 --data coco.yaml --img 1280 1280 --cfg yolov7-w6.yaml\n",
    "--weights ''  --name yolov7-w6 --hyp hyp.scratch.p6.yaml \n",
    "--n-training-data 100 --n-val-data 20 --correct-exif\n",
    "\"\"\"\n",
    "args=parser.parse_args(argument.split())\n",
    "\n",
    "device=torch.device('cpu' if not torch.cuda.is_available() or args.device=='cpu' else 'cuda')\n",
    "print(device, args.batch_size)\n",
    "\n",
    "# hyperparameters\n",
    "with open(args.hyp) as f: hyp=yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad98e754-be7c-4ca3-8992-c7a82ed0438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 3\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n"
     ]
    }
   ],
   "source": [
    "nc=80\n",
    "# define model and optimizers\n",
    "model=Model(args.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # it is safer to move model to device first and then create optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37eb6a44-36d4-493a-a991-d876de2a39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset.coco_dataset.__init__ save cache to D:\\data\\coco\\coco\\labels\\train2017.cache cache_path.is_file() True\n"
     ]
    }
   ],
   "source": [
    "with open(args.data) as f: data_dict=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# train/val data loader\n",
    "train_dataset=LoadImagesAndLabels(data_dirpath=args.data_dirpath, image_paths=data_dict['train'], img_size=args.img_size[0],\n",
    "                            augment=True, hyp=hyp, n_data=args.n_training_data, correct_exif=args.correct_exif)\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, num_workers=1, pin_memory=True, \n",
    "                                        collate_fn=LoadImagesAndLabels.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc7a55c-fa7c-4759-ba7b-6166314fa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from video_processing.yolov7.loss.module import ComputeLossAuxOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71df158-88c2-442d-affd-f81fdcaa239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.3 0.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# blending factor between fixed objectness of 1 and IoU between prediction and ground truth\n",
    "# used to set target objectness, i.e., target_objectness = (1-gr)+gr*iou\n",
    "model.gr=1.\n",
    "loss_module=ComputeLossAuxOTA(model, cls_pw=hyp['cls_pw'], obj_pw=hyp['obj_pw'], label_smoothing=args.label_smoothing)\n",
    "print(hyp['obj'], hyp['cls'], hyp['box'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9931e9da-f744-4f7b-89ba-035d796f30e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs  torch.Size([2, 3, 1280, 1280]) torch.uint8 0 154\n",
      "targets  torch.Size([4, 6]) torch.float32 ['0.00', '0.00', '0.50', '0.40', '0.06', '0.06'] ['1.00', '32.00', '0.67', '0.71', '1.00', '0.60']\n"
     ]
    }
   ],
   "source": [
    "imgs, targets, paths=next(iter(train_loader))\n",
    "print('imgs ', imgs.shape, imgs.dtype, imgs.min().item(), imgs.max().item())\n",
    "print('targets ', targets.shape, targets.dtype, [f'{x:.2f}' for x in targets.min(0).values.tolist()], \n",
    "      [f'{x:.2f}' for x in targets.max(0).values.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2432d4-b32e-4221-b77f-e62a54a53cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.Tensor'> 8 [torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85]), torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85])]\n"
     ]
    }
   ],
   "source": [
    "imgs=imgs.to(device, non_blocking=True).float() / 255.0\n",
    "predictions = model(imgs)  # forward\n",
    "print(type(predictions), type(predictions[0]), len(predictions), [p.shape for p in predictions])\n",
    "#loss, loss_items = compute_loss_ota(pred, targets.to(device), imgs)  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac543cf4-e526-4185-87e8-9b2b4259a9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.4002], grad_fn=<MulBackward0>),\n",
       " tensor([0.1461, 0.9509, 0.1031, 1.2001]),\n",
       " tensor([2.9214, 1.3585, 0.3436, 1.2001]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_module(predictions, targets, images=imgs,matching_threshold=hyp['anchor_t'],box_weight=hyp['box'], obj_weight=hyp['obj'], cls_weight=hyp['cls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e637fba-0999-4ad8-bf2a-3547a58920d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76131e-b1f1-4f8b-ba15-d565fb1b17c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1004d99-8dd8-42bf-b28d-5c8b66ea16aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "48bc854d-3e31-4fc8-877a-fa0d28ed27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85]), torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1280, 1280])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(predictions), [p.shape for p in predictions])\n",
    "len(targets), [t.shape for t in targets]\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b9f11b-c895-4a42-ad4e-f5f5f1f0d6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1280, 1280])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions (list[Tensor]): list of 2NL BxAxHxWxO output where NL is the number of levels, (2 for output from main and auxillary heads)\n",
    "targets (list[Tensor]): list of Ntx6 targets per level, where Nt is the number of targets which may vary per level and 6 for image-index,\n",
    "    class-index, x,y,w,h in normalized space relative to image width and height. (x,y) is the box center\n",
    "imgs (Tensor[float]): BxCxHxW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac784ae4-e357-4f19-a0d0-1a22e4934343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def __call__(self, predictions, targets, images):\n",
    "# bs_aux, as_aux_, gjs_aux, gis_aux, targets_aux, anchors_aux = self.build_targets2(p[:self.nl], targets, imgs)\n",
    "# find the targets in cell grid unit that match anchors for training auxillary head\n",
    "indices4aux, anch4aux =find_5_positive(prediction=predictions[:loss_module.nl], targets=targets, anchors=loss_module.anchors,\n",
    "                                             matching_threshold=hyp['anchor_t'], inside_grid_cell=1.)\n",
    "bs_aux, as_aux_, gjs_aux, gis_aux, targets_aux, anchors_aux \\\n",
    "=determine_matching_targets(prediction=predictions[:loss_module.nl], targets=targets, indices=indices4aux, anch=anch4aux,\n",
    "                                               stride=loss_module.stride, image_size=imgs.shape[2], n_classes=loss_module.nc)\n",
    "\n",
    "indices4main, anch4main=find_5_positive(prediction=predictions[:loss_module.nl], targets=targets, anchors=loss_module.anchors,\n",
    "                                             matching_threshold=hyp['anchor_t'], inside_grid_cell=.5)\n",
    "bs, as_,gjs, gis, targets, anchors \\\n",
    "=determine_matching_targets(prediction=pred[:loss_module.nl], targets=targets, indices=indices4main, anch=anch4main,\n",
    "                                               stride=loss_module.stride, image_size=imgs.shape[2], n_classes=loss_module.nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2945e678-7535-4b6d-8191-1d1807e3f4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([160, 160, 160, 160]), tensor([80, 80, 80, 80]), tensor([40, 40, 40, 40]), tensor([20, 20, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "# list of 1D of WHWH (XYXY) grid size per level\n",
    "feature_grid_resolution=[torch.tensor(pred.shape, device=device)[[3,2,3,2]] for pred in predictions[:loss_module.nl]]\n",
    "print(feature_grid_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95c57066-9164-4e3b-a103-11d91fc0234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ----------------------------------------------------------------------------------------------------\n",
      "1 ----------------------------------------------------------------------------------------------------\n",
      "2 ----------------------------------------------------------------------------------------------------\n",
      "3 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2.4615], grad_fn=<MulBackward0>),\n",
       " tensor([0.1696, 0.9511, 0.1101, 1.2307]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_loss=torch.zeros(1, device=device)\n",
    "class_loss=torch.zeros(1, device=device)\n",
    "objectness_loss=torch.zeros(1, device=device) # objectness\n",
    "for level in range(loss_module.nl):\n",
    "    print(level, '-'*100)\n",
    "    pred_l=predictions[level] # BxAxHxWxO prediction for level l \n",
    "    pred_aux_l=predictions[loss_module.nl+level] # BxAxHxWxO prediction from auxillary head for level l \n",
    "    # image-index, anchor-index, grid-j, grid-i\n",
    "    b, a, gj, gi=bs[level], as_[level], gjs[level], gis[level] # all 1D long indices\n",
    "    b_aux,a_aux, gj_aux, gi_aux=bs_aux[level], as_aux_[level], gjs_aux[level], gis_aux[level] # all 1D long indices\n",
    "    target_objectness=torch.zeros_like(pred_l[...,0], device=device) # BxAxHxW\n",
    "    target_objectness_aux=torch.zeros_like(pred_aux_l[...,0], device=device) # BxAxHxW\n",
    "    \n",
    "    n_targets=b.shape[0]\n",
    "    if n_targets>0:\n",
    "        # predictions corresponding to targets\n",
    "        positive_pred_l=pred_l[b,a,gj,gi] # n_targets x O\n",
    "        iou, iou_loss=box_regression(positive_pred_l[:,:4], target_boxes=targets[level][:,2:]*feature_grid_resolution[level][None], \n",
    "               grid_cell=torch.stack([gi, gj], dim=1), anchors=anchors[level])\n",
    "        box_loss+=iou_loss\n",
    "\n",
    "        # BxAxHxW target objectness blending iou and fixed objectness of 1\n",
    "        target_objectness[b,a,gj,gi]=(1.-loss_module.gr)+loss_module.gr*iou.detach().clamp(min=0.).type(target_objectness.dtype)\n",
    "\n",
    "        # classification\n",
    "        if loss_module.nc>1: # only for multiple classes\n",
    "            class_loss+=multilabel_classification_loss(predictions=positive_pred_l[:,5:], target_class_indices=targets[level][:,1].long(), \n",
    "                                           pos_weight=loss_module.class_positive_weight, pos_value=loss_module.positive_class, \n",
    "                                           neg_value=loss_module.negative_class)\n",
    "    n_aux=b_aux.shape[0] # number of target for auxillary head\n",
    "    if n_aux>0:\n",
    "        positive_pred_aux_l=pred_aux_l[b_aux, a_aux, gj_aux, gi_aux]\n",
    "        iou_aux, iou_aux_loss=box_regression(positive_pred_aux_l[:,:4], target_boxes=targets_aux[level][:,2:]*feature_grid_resolution[level][None], \n",
    "                       grid_cell=torch.stack([gi_aux, gj_aux], dim=1), anchors=anchors_aux[level])\n",
    "        box_loss+=0.25*iou_aux_loss\n",
    "\n",
    "        # objectness target\n",
    "        target_objectness_aux[b_aux, a_aux, gj_aux, gi_aux]=(1.-loss_module.gr) + \\\n",
    "        loss_module.gr*iou_aux.detach().clamp(0).type(target_objectness_aux.dtype)\n",
    "\n",
    "        if loss_module.nc>1:\n",
    "            class_loss+=0.25*multilabel_classification_loss(predictions=positive_pred_aux_l[:,5:], target_class_indices=targets_aux[level][:,1].long(), \n",
    "                               pos_weight=loss_module.class_positive_weight, pos_value=loss_module.positive_class, \n",
    "                               neg_value=loss_module.negative_class)\n",
    "    # objectness losses\n",
    "    obj_main_loss=F.binary_cross_entropy_with_logits(input=pred_l[...,4], target=target_objectness, pos_weight=loss_module.object_positive_weight)\n",
    "    obj_aux_loss=F.binary_cross_entropy_with_logits(input=pred_aux_l[...,4], target=target_objectness_aux, pos_weight=loss_module.object_positive_weight)\n",
    "    objectness_loss+=loss_module.balance[level]*(obj_main_loss+0.25*obj_aux_loss)\n",
    "    \n",
    "\n",
    "loss=loss_module.box_weight*box_loss+loss_module.cls_weight*class_loss+loss_module.obj_weight*objectness_loss\n",
    "float(imgs.shape[0])*loss, torch.cat((loss_module.box_weight*box_loss, loss_module.obj_weight*objectness_loss, loss_module.cls_weight*class_loss, loss)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6685bba6-a8fe-484e-b97b-79c1d94a79cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3916, 1.3587, 0.3669, 1.2307])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((box_loss, objectness_loss, class_loss, loss)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "558d5d00-6bf0-456b-bdc6-00e8729816b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 80]), torch.int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        self.object_positive_weight=torch.tensor([obj_pw], device=device)\n",
    "        # self.BCEcls=nn.BCEWithLogitsLoss(pos_weight=torch.tensor([cls_pw], device=device))\n",
    "        # self.BCEobj=nn.BCEWithLogitsLoss(pos_weight=torch.tensor([obj_pw], device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9dc3bd6e-9fa0-40d7-8377-3244a2c5f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0815, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "multilabel_classification_loss(predictions=positive_pred_l[:,5:], target_class_indices=targets[level][:,1].long(), \n",
    "                               pos_weight=loss_module.class_positive_weight, pos_value=loss_module.positive_class, \n",
    "                               neg_value=loss_module.negative_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5d8bf44-b77d-4f3a-840f-67e83e687550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0815], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5cd920e2-9082-4198-83d3-60bfda47231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True tensor(0.6744, grad_fn=<DivBackward0>) tensor([0.6744], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "        self.class_positive_weight=cls_pw\n",
    "        self.object_positive_weight=obj_pw\n",
    "\n",
    "nn.BCEWithLogitsLoss(pos_weight=torch.tensor([cls_pw], device=device))\n",
    "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "06e713cf-ce2c-4a67-952b-df5e2c049cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True tensor(0.2522, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "            # here we do not need to multiply p_obj and p_cls since we only compute the classification loss\n",
    "            # for positive samples (objects exist)\n",
    "            target_class_indices=targets[level][:,1].long() # n_targets\n",
    "            # n_targets x n_classes\n",
    "            target_classes=torch.full_like(positive_pred_l[:,5:], loss_module.negative_class, device=device)\n",
    "            target_classes[range(n_targets), target_class_indices]=loss_module.positive_class # one-hot\n",
    "            class_loss+=loss_module.BCEcls(positive_pred_l[:,5:], target_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
