{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c9fb1a-c884-451e-8762-5df7b04af72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f67fe60c30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed=0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "## slower, more reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = False, True\n",
    "## faster, less reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcc5aa3-578e-40a3-ae0b-8520deb57c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from video_processing.yolov7.parameter_parser import parser\n",
    "from video_processing.yolov7.models.model import Model\n",
    "from video_processing.yolov7.train.utils import setup_optimizer, labels_to_class_weights\n",
    "from video_processing.yolov7.dataset.coco_dataset import LoadImagesAndLabels\n",
    "from video_processing.yolov7.utils.general import one_cycle, check_image_size\n",
    "from video_processing.yolov7.dataset.anchors import check_anchor_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585e7804-1a1c-4227-90cc-e75b1e22cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 2\n"
     ]
    }
   ],
   "source": [
    "data_dirpath='D:/data/coco'\n",
    "result_dirpath='D:/results/yolov7'\n",
    "\n",
    "argument=f\"\"\"\n",
    "--data-dirpath {data_dirpath}/coco --output-dirpath {result_dirpath} \n",
    "--worker 1 --device cpu --batch-size 2 --data coco.yaml --img 1280 1280 --cfg yolov7-w6.yaml\n",
    "--weights ''  --name yolov7-w6 --hyp hyp.scratch.p6.yaml \n",
    "--n-training-data 100 --n-val-data 20 --correct-exif\n",
    "\"\"\"\n",
    "args=parser.parse_args(argument.split())\n",
    "\n",
    "device=torch.device('cpu' if not torch.cuda.is_available() or args.device=='cpu' else 'cuda')\n",
    "print(device, args.batch_size)\n",
    "\n",
    "# hyperparameters\n",
    "with open(args.hyp) as f: hyp=yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad98e754-be7c-4ca3-8992-c7a82ed0438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 3\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n"
     ]
    }
   ],
   "source": [
    "nc=80\n",
    "# define model and optimizers\n",
    "model=Model(args.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # it is safer to move model to device first and then create optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37eb6a44-36d4-493a-a991-d876de2a39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset.coco_dataset.__init__ save cache to D:\\data\\coco\\coco\\labels\\train2017.cache cache_path.is_file() True\n"
     ]
    }
   ],
   "source": [
    "with open(args.data) as f: data_dict=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# train/val data loader\n",
    "train_dataset=LoadImagesAndLabels(data_dirpath=args.data_dirpath, image_paths=data_dict['train'], img_size=args.img_size[0],\n",
    "                            augment=True, hyp=hyp, n_data=args.n_training_data, correct_exif=args.correct_exif)\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, num_workers=1, pin_memory=True, \n",
    "                                        collate_fn=LoadImagesAndLabels.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc7a55c-fa7c-4759-ba7b-6166314fa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_processing.yolov7.loss.utils import find_5_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71df158-88c2-442d-affd-f81fdcaa239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLossAuxOTA:\n",
    "    def __init__(self, model, cls_pw, obj_pw, label_smoothing):\n",
    "        '''\n",
    "        Args:\n",
    "            cls_pw (float/sequence): positive weight for class classification in BCEWithLogitsLoss\n",
    "            obj_pw (float/sequence): positive weight for objectness in BCEWithLogitsLoss\n",
    "            label_smoothing (float): label smoothing eps\n",
    "        '''\n",
    "        super(ComputeLossAuxOTA, self).__init__()\n",
    "        device=next(model.parameters()).device\n",
    "        self.BCEcls=nn.BCEWithLogitsLoss(pos_weight=torch.tensor([cls_pw], device=device))\n",
    "        self.BCEobj=nn.BCEWithLogitsLoss(pos_weight=torch.tensor([obj_pw], device=device))\n",
    "\n",
    "        # positive and negative class\n",
    "        # https://arxiv.org/pdf/1902.04103.pdf eqn 3\n",
    "        # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441\n",
    "        self.positive_class, self.negative_class=1.0 - 0.5*label_smoothing, 0.5*label_smoothing\n",
    "\n",
    "        self.balance=[4.,1.,0.4] if model.model[-1].nl<4 else [4., 1., .25, .06, .02]\n",
    "\n",
    "        self.ssi=0 #?\n",
    "        self.gr=model.gr #?\n",
    "        for k in 'na,nc,nl,anchors,stride'.split(','): setattr(self, k, getattr(model.model[-1], k))\n",
    "            \n",
    "model.gr=1. # iou loss ratio (obj_loss =1 or iou)?\n",
    "loss_module=ComputeLossAuxOTA(model, cls_pw=hyp['cls_pw'], obj_pw=hyp['obj_pw'], label_smoothing=args.label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9931e9da-f744-4f7b-89ba-035d796f30e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs  torch.Size([2, 3, 1280, 1280]) torch.uint8 0 154\n",
      "targets  torch.Size([4, 6]) torch.float32 ['0.00', '0.00', '0.50', '0.40', '0.06', '0.06'] ['1.00', '32.00', '0.67', '0.71', '1.00', '0.60']\n"
     ]
    }
   ],
   "source": [
    "imgs, targets, paths=next(iter(train_loader))\n",
    "print('imgs ', imgs.shape, imgs.dtype, imgs.min().item(), imgs.max().item())\n",
    "print('targets ', targets.shape, targets.dtype, [f'{x:.2f}' for x in targets.min(0).values.tolist()], \n",
    "      [f'{x:.2f}' for x in targets.max(0).values.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2432d4-b32e-4221-b77f-e62a54a53cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.Tensor'> 8 [torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85]), torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85])]\n"
     ]
    }
   ],
   "source": [
    "imgs=imgs.to(device, non_blocking=True).float() / 255.0\n",
    "pred = model(imgs)  # forward\n",
    "print(type(pred), type(pred[0]), len(pred), [p.shape for p in pred])\n",
    "#loss, loss_items = compute_loss_ota(pred, targets.to(device), imgs)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48bc854d-3e31-4fc8-877a-fa0d28ed27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85]), torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 3, 80, 80, 85]), torch.Size([2, 3, 40, 40, 85]), torch.Size([2, 3, 20, 20, 85])]\n"
     ]
    }
   ],
   "source": [
    "print(len(pred), [p.shape for p in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b9f11b-c895-4a42-ad4e-f5f5f1f0d6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1280, 1280])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac784ae4-e357-4f19-a0d0-1a22e4934343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from video_processing.yolov7.loss.utils import determine_matching_target_for_auxillary_heads, find_5_positive\n",
    "\n",
    "# def __call__(self, predictions, targets, images):\n",
    "# bs_aux, as_aux_, gjs_aux, gis_aux, targets_aux, anchors_aux = self.build_targets2(p[:self.nl], targets, imgs)\n",
    "# find the targets in cell grid unit that match anchors for training auxillary head\n",
    "indices4aux, anch4aux =find_5_positive(prediction=pred[:loss_module.nl], targets=targets, anchors=loss_module.anchors,\n",
    "                                             matching_threshold=hyp['anchor_t'], inside_grid_cell=1.)\n",
    "bs_aux, as_aux_, gjs_aux, gis_aux, targets_aux, anchors_aux \\\n",
    "=determine_matching_target_for_auxillary_heads(prediction=pred[:loss_module.nl], targets=targets, indices=indices4aux, anch=anch4aux,\n",
    "                                               stride=loss_module.stride, image_size=imgs.shape[2], n_classes=loss_module.nc)\n",
    "\n",
    "indices4main, anch4main=find_5_positive(prediction=pred[:loss_module.nl], targets=targets, anchors=loss_module.anchors,\n",
    "                                             matching_threshold=hyp['anchor_t'], inside_grid_cell=.5)\n",
    "bs, as_,gjs, gis, targets, anchors \\\n",
    "=determine_matching_target_for_auxillary_heads(prediction=pred[:loss_module.nl], targets=targets, indices=indices4main, anch=anch4main,\n",
    "                                               stride=loss_module.stride, image_size=imgs.shape[2], n_classes=loss_module.nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945e678-7535-4b6d-8191-1d1807e3f4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c57066-9164-4e3b-a103-11d91fc0234d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ea6117-3311-4888-ad59-dcf7c9b2edbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19698ae7-62f7-4066-94f4-b4edee84b652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "399f7e02-ddf1-4bde-87f7-25a95d2277b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd920e2-9082-4198-83d3-60bfda47231c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1]),\n",
       " tensor([1, 1, 1, 1, 1]),\n",
       " tensor([0, 1, 1, 1]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_bs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
