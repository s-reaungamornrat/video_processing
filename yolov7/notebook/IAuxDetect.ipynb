{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8461e6d8-caf3-4b5d-a1b4-0413704a2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from video_processing.yolov7.utils.general import make_grid\n",
    "from video_processing.yolov7.models.common import Conv, autopad, ImplicitA, ImplicitM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9b7494-efff-456d-aa03-8d8dd6bbbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAuxDetect(nn.Module):\n",
    "    # class variables shared across all instances\n",
    "    stride=None # compute during build\n",
    "    export = False # onnx export\n",
    "    end2end=False\n",
    "    include_nms=False\n",
    "    concat=False\n",
    "    def __init__(self, nc=80, anchors=(), ch=()):# detection layer\n",
    "        '''\n",
    "        Args:\n",
    "            nc (int): number of object classes\n",
    "            anchors (list[list[int]]): 3 pairs of anchor width/heights for small, medium, and large bounding boxes per level,\n",
    "                e.g., [[19,27,  44,40,  38,94], \n",
    "                       [96,68,  86,152,  180,137], \n",
    "                       [140,301,  303,264,  238,542], \n",
    "                       [436,615,  739,380,  925,792]]\n",
    "            ch (list[int]): list of input channels for each level for its m and m2 modules, e.g., \n",
    "                [256, 512, 768, 1024, 320, 640, 960, 1280] where the first 4 are input channels for \n",
    "                each level of m and the remainings are the same for m2\n",
    "        '''\n",
    "        super(IAuxDetect, self).__init__()\n",
    "        self.nc=nc # number of classes\n",
    "        self.no=nc+5 # number of output per anchors\n",
    "        self.nl=len(anchors) # number of detection layers\n",
    "        self.na=len(anchors[0])//2 # number of anchors\n",
    "        self.grid=[torch.zeros(1)]*self.nl # init grid\n",
    "        a=torch.tensor(anchors).float().view(self.nl, -1, 2)\n",
    "        self.register_buffer('anchors', a) # shape nl, na, 2 \n",
    "        # nl 1 na 1 1 2 \n",
    "        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))\n",
    "        print(f'In IAxDetect nl: {self.nl} na: {self.na}')\n",
    "        print(f'In IAxDetect anchors: {self.anchors.shape} {self.nl}x{self.na}x{2}')\n",
    "        print(f'In IAxDetect anchor_grid: {self.anchor_grid.shape} {self.nl}x1x{self.na}x1x1x{2}')\n",
    "        self.m=nn.ModuleList(nn.Conv2d(x, self.no*self.na, 1) for x in ch[:self.nl]) # output conv\n",
    "        self.m2=nn.ModuleList(nn.Conv2d(x, self.no*self.na, 1) for x in ch[self.nl:]) # output conv\n",
    "\n",
    "        self.ia=nn.ModuleList(ImplicitA(x) for x in ch[:self.nl])\n",
    "        self.im=nn.ModuleList(ImplicitM(self.no*self.na) for _ in ch[:self.nl])\n",
    "    def forward(self, x, verbose=False):\n",
    "        '''\n",
    "        Args:\n",
    "            x (list[Tensor])\n",
    "        '''\n",
    "        ## see https://github.com/WongKinYiu/yolov7/blob/main/models/yolo.py#L116\n",
    "        z=[]\n",
    "        self.training|=self.export\n",
    "        for i in range(self.nl):\n",
    "            if verbose:\n",
    "                print(i, '-'*100)\n",
    "                print('\\tx[i] ', x[i].shape,  ' detector.m[i] ', detector.m[i])\n",
    "            x[i]=self.m[i](self.ia[i](x[i]))\n",
    "            x[i]=self.im[i](x[i])\n",
    "            if verbose: print('\\tx[i] ', x[i].shape)\n",
    "            bs,_,ny,nx=x[i].shape\n",
    "            # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "            x[i]=x[i].view(bs, self.na, self.no, ny, nx).permute(0,1,3,4,2).contiguous() \n",
    "            if verbose:print('\\tx[i] ', x[i].shape,'\\n\\ti+nl ', i+self.nl)\n",
    "        \n",
    "            if verbose: print('\\tx[i+self.nl] ', x[i+self.nl].shape,  ' self.m2[i] ', self.m2[i])\n",
    "            x[i+self.nl]=self.m2[i](x[i+self.nl])\n",
    "            if verbose: print('\\tx[i+self.nl] ', x[i+self.nl].shape)\n",
    "            # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "            x[i+self.nl]=x[i+self.nl].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "            if verbose: print('\\tx[i+self.nl] ', x[i+self.nl].shape)\n",
    "            if not self.training: # inference\n",
    "                if verbose:\n",
    "                    print('\\tself.grid[i].shape ', self.grid[i].shape)\n",
    "                    print('\\tself.grid[i].shape[2:4] ', self.grid[i].shape[2:4], ' x[i].shape[2:4] ', x[i].shape[2:4])\n",
    "                if self.grid[i].shape[2:4]!=x[i].shape[2:4]:\n",
    "                    # feature map grid\n",
    "                    self.grid[i]=make_grid(nx=nx, ny=ny).to(x[i].device) # 1x1xHxWx2\n",
    "                if verbose: print('\\tself.grid[i].shape ', self.grid[i].shape )\n",
    "                y=x[i].sigmoid()\n",
    "                if verbose: \n",
    "                    print('\\tx[i] ', x[i].min().item(), x[i].max().item())\n",
    "                    print('\\ty ', y.shape, y.min().item(), y.max().item() )\n",
    "                    print('\\tstride ', self.stride[i])\n",
    "                if not torch.onnx.is_in_onnx_export():\n",
    "                    # xy coordinates of bounding boxes\n",
    "                    #               BxAxHxWx2        1x1xHxWx2  \n",
    "                    y[...,0:2]=(2.*y[...,0:2] -0.5 + self.grid[i])*self.stride[i]\n",
    "                    # width and height of bounding boxes\n",
    "                    #              BxAxHxWx2             1xAx1x1x2\n",
    "                    y[...,2:4]=( (2.*y[...,2:4])**2. ) * self.anchor_grid[i]\n",
    "                else:\n",
    "                    # split the 4th dim of BxAxHxWxO into BxAxHxWx2 of xy, BxAxHxWx2 of width/height, and BxAxHxWx(Nc+1) of objectness and Nc for number of classes\n",
    "                    xy,wh,confidence=y.split((2,2,self.nc+1), dim=4)\n",
    "                    if verbose: print('xy ', xy.shape, ' wh ', wh.shape, ' confidence ', confidence.shape)\n",
    "                    xy=xy*(2.* self.stride[i])+(self.stride[i]*(self.grid[i]-0.5))\n",
    "                    wh=wh**2. * (4.*self.anchor_grid[i].data)\n",
    "                    y=torch.cat([xy,wh,confidence], dim=4)\n",
    "                    \n",
    "                z.append(y.view(bs, -1, self.no)) # BxAxHxWxO -> Bx(AHW)xO\n",
    "                \n",
    "        return x if self.training else (torch.cat(z, 1), x[:self.nl])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620745c5-56c0-4cfa-9db9-28e4749faef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 3\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n",
      "len(x)  8  nl  4  na  3  no  85\n"
     ]
    }
   ],
   "source": [
    "nc=80\n",
    "anchors=[[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]]\n",
    "ch=[256, 512, 768, 1024, 320, 640, 960, 1280]\n",
    "detector=IAuxDetect(nc=nc, anchors=anchors, ch=ch)\n",
    "detector.stride=torch.tensor([ 8., 16., 32., 64.], dtype=torch.float32)\n",
    "\n",
    "detector.eval()\n",
    "x=[torch.rand(size) for size in [torch.Size([2, 256, 160, 160]), torch.Size([2, 512, 80, 80]), torch.Size([2, 768, 40, 40]), \n",
    "                                 torch.Size([2, 1024, 20, 20]), torch.Size([2, 320, 160, 160]), torch.Size([2, 640, 80, 80]), \n",
    "                                 torch.Size([2, 960, 40, 40]), torch.Size([2, 1280, 20, 20])] ]\n",
    "print('len(x) ', len(x), ' nl ', detector.nl, ' na ',  detector.na, ' no ',  detector.no)\n",
    "output=detector(x)\n",
    "out0, out1=output\n",
    "print(type(out0), type(out1))\n",
    "out0.shape,[i.shape for i in out1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5968f8df-7d29-49e9-b553-72a28adf0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x)  8  nl  4  na  3  no  85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 160, 160, 85]),\n",
       " torch.Size([2, 3, 80, 80, 85]),\n",
       " torch.Size([2, 3, 40, 40, 85]),\n",
       " torch.Size([2, 3, 20, 20, 85]),\n",
       " torch.Size([2, 3, 160, 160, 85]),\n",
       " torch.Size([2, 3, 80, 80, 85]),\n",
       " torch.Size([2, 3, 40, 40, 85]),\n",
       " torch.Size([2, 3, 20, 20, 85])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.train()\n",
    "x=[torch.rand(size) for size in [torch.Size([2, 256, 160, 160]), torch.Size([2, 512, 80, 80]), torch.Size([2, 768, 40, 40]), \n",
    "                                 torch.Size([2, 1024, 20, 20]), torch.Size([2, 320, 160, 160]), torch.Size([2, 640, 80, 80]), \n",
    "                                 torch.Size([2, 960, 40, 40]), torch.Size([2, 1280, 20, 20])] ]\n",
    "print('len(x) ', len(x), ' nl ', detector.nl, ' na ',  detector.na, ' no ',  detector.no)\n",
    "output=detector(x)\n",
    "[i.shape for i in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7e29a-8cf2-4aa0-b12c-cbee2b1e31e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8cd84a-e08f-4f1b-ab11-4e9fda872128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x)  8  nl  4  na  3  no  85\n",
      "False\n",
      "0 ----------------------------------------------------------------------------------------------------\n",
      "\tx[i]  torch.Size([2, 256, 160, 160])  detector.m[i]  Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tx[i]  torch.Size([2, 255, 160, 160])\n",
      "\tx[i]  torch.Size([2, 3, 160, 160, 85])\n",
      "\ti+nl  4\n",
      "\tx[i+detector.nl]  torch.Size([2, 320, 160, 160])  detector.m2[i]  Conv2d(320, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tx[i+detector.nl]  torch.Size([2, 255, 160, 160])\n",
      "\tx[i+detector.nl]  torch.Size([2, 3, 160, 160, 85])\n",
      "\tdetector.grid[i].shape  torch.Size([1])\n",
      "\tdetector.grid[i].shape[2:4]  torch.Size([])  x[i].shape[2:4]  torch.Size([160, 160])\n",
      "\tdetector.grid[i].shape  torch.Size([1, 1, 160, 160, 2])\n",
      "\tx[i]  -1.5335314273834229 1.637326955795288\n",
      "\ty  torch.Size([2, 3, 160, 160, 85]) 0.17747758328914642 0.8371708989143372\n",
      "\tstride  tensor(8.)\n",
      "[torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 512, 80, 80]), torch.Size([2, 768, 40, 40]), torch.Size([2, 1024, 20, 20]), torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 640, 80, 80]), torch.Size([2, 960, 40, 40]), torch.Size([2, 1280, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "detector.eval()\n",
    "x=[torch.rand(size) for size in [torch.Size([2, 256, 160, 160]), torch.Size([2, 512, 80, 80]), torch.Size([2, 768, 40, 40]), \n",
    "                                 torch.Size([2, 1024, 20, 20]), torch.Size([2, 320, 160, 160]), torch.Size([2, 640, 80, 80]), \n",
    "                                 torch.Size([2, 960, 40, 40]), torch.Size([2, 1280, 20, 20])] ]\n",
    "\n",
    "z=[]\n",
    "print(detector.training|detector.export)\n",
    "for i in range(detector.nl):\n",
    "    print(i, '-'*100)\n",
    "    print('\\tx[i] ', x[i].shape,  ' detector.m[i] ', detector.m[i])\n",
    "    x[i]=detector.m[i](detector.ia[i](x[i]))\n",
    "    x[i]=detector.im[i](x[i])\n",
    "    print('\\tx[i] ', x[i].shape)\n",
    "    bs,_,ny,nx=x[i].shape\n",
    "    # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "    x[i]=x[i].view(bs, detector.na, detector.no, ny, nx).permute(0,1,3,4,2).contiguous() \n",
    "    print('\\tx[i] ', x[i].shape)\n",
    "    print('\\ti+nl ', i+detector.nl)\n",
    "\n",
    "    print('\\tx[i+detector.nl] ', x[i+detector.nl].shape,  ' detector.m2[i] ', detector.m2[i])\n",
    "    x[i+detector.nl]=detector.m2[i](x[i+detector.nl])\n",
    "    print('\\tx[i+detector.nl] ', x[i+detector.nl].shape)\n",
    "    # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "    x[i+detector.nl]=x[i+detector.nl].view(bs, detector.na, detector.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "    print('\\tx[i+detector.nl] ', x[i+detector.nl].shape)\n",
    "    if not detector.training: # inference\n",
    "        print('\\tdetector.grid[i].shape ', detector.grid[i].shape)\n",
    "        print('\\tdetector.grid[i].shape[2:4] ', detector.grid[i].shape[2:4], ' x[i].shape[2:4] ', x[i].shape[2:4])\n",
    "        if detector.grid[i].shape[2:4]!=x[i].shape[2:4]:\n",
    "            # feature map grid\n",
    "            detector.grid[i]=make_grid(nx=nx, ny=ny).to(x[i].device) # 1x1xHxWx2\n",
    "        print('\\tdetector.grid[i].shape ', detector.grid[i].shape )\n",
    "        y=x[i].sigmoid()\n",
    "        print('\\tx[i] ', x[i].min().item(), x[i].max().item())\n",
    "        print('\\ty ', y.shape, y.min().item(), y.max().item() )\n",
    "        print('\\tstride ', detector.stride[i])\n",
    "        if not torch.onnx.is_in_onnx_export():\n",
    "            # xy coordinates of bounding boxes\n",
    "            #               BxAxHxWx2        1x1xHxWx2  \n",
    "            y[...,0:2]=(2.*y[...,0:2] -0.5 + detector.grid[i])*detector.stride[i]\n",
    "            # width and height of bounding boxes\n",
    "            #              BxAxHxWx2             1xAx1x1x2\n",
    "            y[...,2:4]=( (2.*y[...,2:4])**2. ) * detector.anchor_grid\n",
    "        else:\n",
    "            # split the 4th dim of BxAxHxWxO into BxAxHxWx2 of xy, BxAxHxWx2 of width/height, and BxAxHxWx(Nc+1) of objectness and Nc for number of classes\n",
    "            xy,wh,confidence=y.split((2,2,detector.nc+1), dim=4)\n",
    "            print('xy ', xy.shape, ' wh ', wh.shape, ' confidence ', confidence.shape)\n",
    "            xy=xy*(2.* detector.stride[i])+(detector.stride[i]*(detector.grid[i]-0.5))\n",
    "            wh=wh**2. * (4.*detector.anchor_grid[i].data)\n",
    "            y=torch.cat([xy,wh,confidence], dim=4)\n",
    "        z.append(y.view(bs, -1, detector.no)) # BxAxHXWxO -> Bx(AHW)xO\n",
    "    break\n",
    "print([v.shape for v in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c401f0-25fc-4a14-8644-290768139463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.stride[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb7aab8-a4c4-482e-933f-47cc42d3fedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef7af37-8d02-475a-97ac-1280064283b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 160, 160, 85]), torch.Size([2, 76800, 85]), 85)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, y.view(bs, -1, detector.no).shape, detector.no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5570766e-2f98-4d2b-80b8-718cd6aaa345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 20]) torch.Size([50, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50, 20, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                if not torch.onnx.is_in_onnx_export():\n",
    "                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n",
    "                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
    "                else:\n",
    "                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0\n",
    "                    xy = xy * (2. * self.stride[i]) + (self.stride[i] * (self.grid[i] - 0.5))  # new xy\n",
    "                    wh = wh ** 2 * (4 * self.anchor_grid[i].data)  # new wh\n",
    "                    y = torch.cat((xy, wh, conf), 4)\n",
    "                z.append(y.view(bs, -1, self.no))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
