{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f657275-18fb-4738-88e7-e72de8795b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23880387bf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed=0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "## slower, more reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = False, True\n",
    "## faster, less reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e702822-4ff5-4a6b-b268-875d53ea133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from video_processing.yolov7.parameter_parser import parser\n",
    "from video_processing.yolov7.models.model import Model\n",
    "from video_processing.yolov7.models.ema import ModelEMA\n",
    "from video_processing.yolov7.loss.module import ComputeLoss\n",
    "from video_processing.yolov7.train.utils import setup_optimizer, labels_to_class_weights, train_an_epoch\n",
    "from video_processing.yolov7.dataset.coco_dataset import LoadImagesAndLabels\n",
    "from video_processing.yolov7.utils.general import one_cycle, check_image_size\n",
    "from video_processing.yolov7.dataset.anchors import check_anchor_matching\n",
    "\n",
    "from video_processing.yolov7.metrics.confusion_matrix import ConfusionMatrix\n",
    "from video_processing.yolov7.utils.general import non_max_suppression\n",
    "from video_processing.yolov7.dataset.coords import adjust_coords, xywh2xyxy\n",
    "from video_processing.yolov7.test.utils import statistics_per_image, validation\n",
    "from video_processing.yolov7.metrics.utils import ap_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c805a3-f147-4a1f-b703-2e82f47b987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 2\n"
     ]
    }
   ],
   "source": [
    "data_dirpath='D:/data/coco'\n",
    "result_dirpath='D:/results/yolov7'\n",
    "\n",
    "argument=f\"\"\"\n",
    "--data-dirpath {data_dirpath}/coco --output-dirpath {result_dirpath} \n",
    "--worker 1 --device cpu --batch-size 2 --data coco.yaml --img 1280 1280 --cfg yolov7-w6.yaml\n",
    "--weights ''  --name yolov7-w6 --hyp hyp.scratch.p6.yaml \n",
    "--n-training-data 100 --n-val-data 20 --correct-exif --print-freq 1 \n",
    "--dev-mode\n",
    "\"\"\"\n",
    "args=parser.parse_args(argument.split())\n",
    "device=torch.device('cpu' if not torch.cuda.is_available() or args.device=='cpu' else 'cuda')\n",
    "print(device, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe065cc1-91ef-4289-9595-5584918c618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.is_coco  True\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "with open(args.hyp) as f: hyp=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# data\n",
    "args.is_coco=len(re.findall(\"coco.yaml$\", args.data))>0\n",
    "print('args.is_coco ', args.is_coco)\n",
    "with open(args.data) as f: data_dict=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# number of classes\n",
    "nc=1 if args.single_cls else int(data_dict['nc']) \n",
    "names=['item'] if args.single_cls and len(data_dict['names'])!=1 else data_dict['names'] # class names\n",
    "assert len(names)==nc, f'There are {len(names)} class names but {nc} classes' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cdc907-2a53-4ae0-8898-10756b2f0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 3\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n",
      "In dataset.coco_dataset.__init__ save cache to D:\\data\\coco\\coco\\labels\\val2017.cache cache_path.is_file() True\n"
     ]
    }
   ],
   "source": [
    "model=Model(args.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # it is safer to move model to device first and then create optimizer\n",
    "# train/val data loader\n",
    "val_dataset=LoadImagesAndLabels(data_dirpath=args.data_dirpath, image_paths=data_dict['val'], img_size=args.img_size[0],\n",
    "                            augment=False, hyp=hyp, n_data=args.n_val_data, correct_exif=args.correct_exif)\n",
    "val_loader=torch.utils.data.DataLoader(dataset=val_dataset, batch_size=args.batch_size, num_workers=1, pin_memory=True, \n",
    "                                        collate_fn=LoadImagesAndLabels.collate_fn)\n",
    "# model_ema=ModelEMA(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b4eb4-1a36-4b0f-96e7-a299f518a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt'\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c8f789-ca10-4136-8d6f-267b16bac8e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m checkpoints\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov7-w6_training.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mtype\u001b[39m(checkpoints), checkpoints\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\op_cv\\lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1361\u001b[0m             opened_zipfile,\n\u001b[0;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   1363\u001b[0m             pickle_module,\n\u001b[0;32m   1364\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1366\u001b[0m         )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\op_cv\\lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\op_cv\\lib\\site-packages\\torch\\serialization.py:1837\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1835\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "_model = torch.hub.load('https://github.com/WongKinYiu/yolov7', 'yolov7-w6')\n",
    "checkpoints=torch.load(os.path.join(args.output_dirpath, 'yolov7-w6_training.pt'),map_location=device, weights_only=False)\n",
    "type(checkpoints), checkpoints.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd52bb-a34b-4973-aeea-867da589b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "nl=model.model[-1].nl\n",
    "print(\"hyp['box'] \", hyp['box'],  \" hyp['cls'] \", hyp['cls'], \" hyp['obj] \", hyp['obj'] )\n",
    "hyp['box']*=3./nl # box-regression loss weight scaled to layer\n",
    "hyp['cls']*= nc/80. * 3./nl # classification loss weight scaled to classes and layers\n",
    "hyp['obj']*=(args.img_size[0]/640)**2 *3./nl # objectness loss weight scaled to image size and layers\n",
    "hyp['label_smoothing']=args.label_smoothing\n",
    "model.nc=nc # attach number of classes to model\n",
    "model.hyp=hyp\n",
    "# blending factor between fixed objectness of 1 and IoU between prediction and ground truth\n",
    "# used to set target objectness, i.e., target_objectness = (1-gr)+gr*iou\n",
    "model.gr=1. \n",
    "model.names=data_dict['names']\n",
    "print(\"hyp['box'] \", hyp['box'],  \" hyp['cls'] \", hyp['cls'], \" hyp['obj] \", hyp['obj'], ' args.label_smoothing ', args.label_smoothing )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
