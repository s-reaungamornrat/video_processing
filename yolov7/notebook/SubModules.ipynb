{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8461e6d8-caf3-4b5d-a1b4-0413704a2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from video_processing.yolov7.utils.general import make_grid\n",
    "from video_processing.yolov7.models.common import Conv, autopad, ImplicitA, ImplicitM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "499eace3-70a3-48f9-a3a6-8858cdd5c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAuxDetect(nn.Module):\n",
    "    # class variables shared across all instances\n",
    "    stride=None # compute during build\n",
    "    export = False # onnx export\n",
    "    end2end=False\n",
    "    include_nms=False\n",
    "    concat=False\n",
    "    def __init__(self, nc=80, anchors=(), ch=()):# detection layer\n",
    "        '''\n",
    "        Args:\n",
    "            nc (int): number of object classes\n",
    "            anchors (sequence): anchors locations\n",
    "            ch (): channel\n",
    "        '''\n",
    "        super(IAuxDetect, self).__init__()\n",
    "        self.nc=nc # number of classes\n",
    "        self.no=nc+5 # number of output per anchors\n",
    "        self.nl=len(anchors) # number of detection layers\n",
    "        self.na=len(anchors[0]) # number of anchors\n",
    "        self.grid=[torch.zeros(1)]*self.nl # init grid\n",
    "        a=torch.tensor(anchors).float().view(self.nl, -1, 2)\n",
    "        self.register_buffer('anchors', a) # shape nl, na//2, 2 but the author said this will be nl,na,2...?\n",
    "        # nl 1 na//2 1 1 2 but the authors said nl 1 na 1 1 2\n",
    "        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))\n",
    "        print(f'In IAxDetect nl: {self.nl} na: {self.na}')\n",
    "        print(f'In IAxDetect anchors: {self.anchors.shape} {self.nl}x{self.na//2}x{2}')\n",
    "        print(f'In IAxDetect anchor_grid: {self.anchor_grid.shape} {self.nl}x1x{self.na//2}x1x1x{2}')\n",
    "        self.m=nn.ModuleList(nn.Conv2d(x, self.no*self.na, 1) for x in ch[:self.nl]) # output conv\n",
    "        self.m2=nn.ModuleList(nn.Conv2d(x, self.no*self.na, 1) for x in ch[self.nl:]) # output conv\n",
    "\n",
    "        self.ia=nn.ModuleList(ImplicitA(x) for x in ch[:self.nl])\n",
    "        self.im=nn.ModuleList(ImplicitM(self.no*self.na) for _ in ch[:self.nl])\n",
    "    def forward(self, x):\n",
    "        ## see https://github.com/WongKinYiu/yolov7/blob/main/models/yolo.py#L116\n",
    "        z=[]\n",
    "        self.training|=self.export\n",
    "        for i in range(self.nl):\n",
    "            print(i, '-'*100)\n",
    "            print('\\tx[i] ', x[i].shape,  ' self.m[i] ', self.m[i])\n",
    "            x[i]=self.m[i](self.ia[i](x[i]))\n",
    "            x[i]=self.im[i](x[i])\n",
    "            print('\\tx[i] ', x[i].shape)\n",
    "            bs,_,ny,nx=x[i].shape\n",
    "            # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "            x[i]=x[i].view(bs, self.na, self.no, ny, nx).permute(0,1,3,4,2).contiguous() \n",
    "            print('\\tx[i] ', x[i].shape)\n",
    "            print('\\ti+nl ', i+self.nl)\n",
    "        \n",
    "            print('\\tx[i+self.nl] ', x[i+self.nl].shape,  ' self.m2[i] ', self.m2[i])\n",
    "            x[i+self.nl]=self.m2[i](x[i+self.nl])\n",
    "            print('\\tx[i+self.nl] ', x[i+self.nl].shape)\n",
    "            # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "            x[i+self.nl]=x[i+self.nl].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "            print('\\tx[i+self.nl] ', x[i+self.nl].shape)\n",
    "            if not self.training: # inference\n",
    "                print('\\tself.grid[i].shape ', self.grid[i].shape)\n",
    "                print('\\tself.grid[i].shape[2:4] ', self.grid[i].shape[2:4], ' x[i].shape[2:4] ', x[i].shape[2:4])\n",
    "                if self.grid[i].shape[2:4]!=x[i].shape[2:4]:\n",
    "                    self.grid[i]=make_grid(nx=nx, ny=ny).to(x[i].device) # 1x1xHxWx2\n",
    "                print('\\tself.grid[i].shape ', self.grid[i].shape )\n",
    "                y=x[i].sigmoid()\n",
    "                print('\\tx[i] ', x[i].min().item(), x[i].max().item())\n",
    "                print('\\ty ', y.min().item(), y.max().item() )\n",
    "                raise NotImplementedError('Please implement after determine stride')\n",
    "        return x if self.training else (torch.cat(z, 1), x[:self.nl])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "620745c5-56c0-4cfa-9db9-28e4749faef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 6\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IAuxDetect(\n",
       "  (m): ModuleList(\n",
       "    (0): Conv2d(256, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(512, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Conv2d(768, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): Conv2d(1024, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (m2): ModuleList(\n",
       "    (0): Conv2d(320, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(640, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Conv2d(960, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): Conv2d(1280, 510, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (ia): ModuleList(\n",
       "    (0-3): 4 x ImplicitA()\n",
       "  )\n",
       "  (im): ModuleList(\n",
       "    (0-3): 4 x ImplicitM()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc=80\n",
    "anchors=[[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]]\n",
    "ch=[256, 512, 768, 1024, 320, 640, 960, 1280]\n",
    "detector=IAuxDetect(nc=nc, anchors=anchors, ch=ch)\n",
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05aaca21-54e5-451f-8d87-45581f8ddc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d8cd84a-e08f-4f1b-ab11-4e9fda872128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x)  8  nl  4  na  6  no  85\n",
      "False\n",
      "0 ----------------------------------------------------------------------------------------------------\n",
      "\tx[i]  torch.Size([2, 256, 160, 160])  detector.m[i]  Conv2d(256, 510, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tx[i]  torch.Size([2, 510, 160, 160])\n",
      "\tx[i]  torch.Size([2, 6, 160, 160, 85])\n",
      "\ti+nl  4\n",
      "\tx[i+detector.nl]  torch.Size([2, 320, 160, 160])  detector.m2[i]  Conv2d(320, 510, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tx[i+detector.nl]  torch.Size([2, 510, 160, 160])\n",
      "\tx[i+detector.nl]  torch.Size([2, 6, 160, 160, 85])\n",
      "\tdetector.grid[i].shape  torch.Size([1, 1, 160, 160, 2])\n",
      "\tdetector.grid[i].shape[2:4]  torch.Size([160, 160])  x[i].shape[2:4]  torch.Size([160, 160])\n",
      "\tdetector.grid[i].shape  torch.Size([1, 1, 160, 160, 2])\n",
      "\tx[i]  -1.5234111547470093 1.5491529703140259\n",
      "\ty  0.17895977199077606 0.8247913718223572\n",
      "[torch.Size([2, 6, 160, 160, 85]), torch.Size([2, 512, 80, 80]), torch.Size([2, 768, 40, 40]), torch.Size([2, 1024, 20, 20]), torch.Size([2, 6, 160, 160, 85]), torch.Size([2, 640, 80, 80]), torch.Size([2, 960, 40, 40]), torch.Size([2, 1280, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "detector.eval()\n",
    "x=[torch.rand(size) for size in [torch.Size([2, 256, 160, 160]), torch.Size([2, 512, 80, 80]), torch.Size([2, 768, 40, 40]), \n",
    " torch.Size([2, 1024, 20, 20]), torch.Size([2, 320, 160, 160]), torch.Size([2, 640, 80, 80]), \n",
    " torch.Size([2, 960, 40, 40]), torch.Size([2, 1280, 20, 20])] ]\n",
    "print('len(x) ', len(x), ' nl ', detector.nl, ' na ',  detector.na, ' no ',  detector.no)\n",
    "\n",
    "z=[]\n",
    "print(detector.training|detector.export)\n",
    "for i in range(detector.nl):\n",
    "    print(i, '-'*100)\n",
    "    print('\\tx[i] ', x[i].shape,  ' detector.m[i] ', detector.m[i])\n",
    "    x[i]=detector.m[i](detector.ia[i](x[i]))\n",
    "    x[i]=detector.im[i](x[i])\n",
    "    print('\\tx[i] ', x[i].shape)\n",
    "    bs,_,ny,nx=x[i].shape\n",
    "    # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "    x[i]=x[i].view(bs, detector.na, detector.no, ny, nx).permute(0,1,3,4,2).contiguous() \n",
    "    print('\\tx[i] ', x[i].shape)\n",
    "    print('\\ti+nl ', i+detector.nl)\n",
    "\n",
    "    print('\\tx[i+detector.nl] ', x[i+detector.nl].shape,  ' detector.m2[i] ', detector.m2[i])\n",
    "    x[i+detector.nl]=detector.m2[i](x[i+detector.nl])\n",
    "    print('\\tx[i+detector.nl] ', x[i+detector.nl].shape)\n",
    "    # BxAxHxWxO where A=number anchors and O is number of classes+5 (where 5 is for bbox coordinate and objectness score) \n",
    "    x[i+detector.nl]=x[i+detector.nl].view(bs, detector.na, detector.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "    print('\\tx[i+detector.nl] ', x[i+detector.nl].shape)\n",
    "    if not detector.training: # inference\n",
    "        print('\\tdetector.grid[i].shape ', detector.grid[i].shape)\n",
    "        print('\\tdetector.grid[i].shape[2:4] ', detector.grid[i].shape[2:4], ' x[i].shape[2:4] ', x[i].shape[2:4])\n",
    "        if detector.grid[i].shape[2:4]!=x[i].shape[2:4]:\n",
    "            detector.grid[i]=make_grid(nx=nx, ny=ny).to(x[i].device) # 1x1xHxWx2\n",
    "        print('\\tdetector.grid[i].shape ', detector.grid[i].shape )\n",
    "        y=x[i].sigmoid()\n",
    "        print('\\tx[i] ', x[i].min().item(), x[i].max().item())\n",
    "        print('\\ty ', y.min().item(), y.max().item() )\n",
    "        raise NotImplementedError('Please implement after determine stride')\n",
    "    break\n",
    "print([v.shape for v in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edb7aab8-a4c4-482e-933f-47cc42d3fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ef7af37-8d02-475a-97ac-1280064283b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 160, 160, 85])\n"
     ]
    }
   ],
   "source": [
    "#if not torch.onnx.is_in_onnx_export():\n",
    "print(y.shape)\n",
    "y[...,0:2]="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5570766e-2f98-4d2b-80b8-718cd6aaa345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 20]) torch.Size([50, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50, 20, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                if not torch.onnx.is_in_onnx_export():\n",
    "                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n",
    "                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
    "                else:\n",
    "                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0\n",
    "                    xy = xy * (2. * self.stride[i]) + (self.stride[i] * (self.grid[i] - 0.5))  # new xy\n",
    "                    wh = wh ** 2 * (4 * self.anchor_grid[i].data)  # new wh\n",
    "                    y = torch.cat((xy, wh, conf), 4)\n",
    "                z.append(y.view(bs, -1, self.no))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
