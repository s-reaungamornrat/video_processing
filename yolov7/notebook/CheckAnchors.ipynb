{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95da31e-eb56-486a-9f50-6f41d3ea7efa",
   "metadata": {},
   "source": [
    "[`check_anchors`](https://github.com/WongKinYiu/yolov7/blob/main/utils/autoanchor.py)\n",
    "\n",
    "Call in [`train_aux.py`](https://github.com/WongKinYiu/yolov7/blob/main/train_aux.py) line 270\n",
    "```\n",
    "    # Anchors\n",
    "    if not opt.noautoanchor:\n",
    "        check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab16af2c-f7f2-4ce2-9864-b300b196def2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2182421ec10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed=0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "## slower, more reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = False, True\n",
    "## faster, less reproducible\n",
    "#cudnn.benchmark, cudnn.deterministic = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5156eb2b-b3f9-4ff7-ab5d-6bda9cf6e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from video_processing.yolov7.parameter_parser import parser\n",
    "from video_processing.yolov7.models.model import Model\n",
    "from video_processing.yolov7.train.utils import setup_optimizer\n",
    "from video_processing.yolov7.dataset.coco_dataset import LoadImagesAndLabels\n",
    "from video_processing.yolov7.utils.general import one_cycle, check_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b58cc0-3df4-4976-8a04-b1ba3ec329c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirpath='D:/data/coco'\n",
    "result_dirpath='D:/results/yolov7'\n",
    "\n",
    "argument=f\"\"\"\n",
    "--data-dirpath {data_dirpath}/coco --output-dirpath {result_dirpath} \n",
    "--worker 1 --device cpu --batch-size 2 --data coco.yaml --img 1280 1280 --cfg yolov7-w6.yaml\n",
    "--weights ''  --name yolov7-w6 --hyp hyp.scratch.p6.yaml \n",
    "--n-training-data 100 --n-val-data 20 --correct-exif\n",
    "\"\"\"\n",
    "args=parser.parse_args(argument.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f593c1c2-41a1-4052-a7da-06067b9def87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 2\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu' if not torch.cuda.is_available() or args.device=='cpu' else 'cuda')\n",
    "print(device, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6800694e-7925-4af3-878d-5680217a6b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.is_coco  True\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "with open(args.hyp) as f: hyp=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# data\n",
    "args.is_coco=len(re.findall(\"coco.yaml$\", args.data))>0\n",
    "print('args.is_coco ', args.is_coco)\n",
    "with open(args.data) as f: data_dict=yaml.load(f, Loader=yaml.SafeLoader)\n",
    "# number of classes\n",
    "nc=1 if args.single_cls else int(data_dict['nc']) \n",
    "names=['item'] if args.single_cls and len(data_dict['names'])!=1 else data_dict['names'] # class names\n",
    "assert len(names)==nc, f'There are {len(names)} class names but {nc} classes' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d36e01-efcb-4f3f-9880-a2cd3e1c7d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IAxDetect nl: 4 na: 3\n",
      "In IAxDetect anchors: torch.Size([4, 3, 2]) 4x3x2\n",
      "In IAxDetect anchor_grid: torch.Size([4, 1, 3, 1, 1, 2]) 4x1x3x1x1x2\n",
      "In dataset.coco_dataset.__init__ save cache to D:\\data\\coco\\coco\\labels\\train2017.cache cache_path.is_file() True\n"
     ]
    }
   ],
   "source": [
    "model=Model(args.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # it is safer to move model to device first and then create optimizer\n",
    "train_dataset=LoadImagesAndLabels(data_dirpath=args.data_dirpath, image_paths=data_dict['train'], img_size=args.img_size[0],\n",
    "                            augment=True, hyp=hyp, n_data=args.n_training_data, correct_exif=args.correct_exif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d852f1b1-5a14-4dc3-ab7a-046cb8e6121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset.anchors.check_anchor_matching anchors/target=5.53, Best possible recall (BPR): 0.9963\n"
     ]
    }
   ],
   "source": [
    "from video_processing.yolov7.dataset.anchors import check_anchor_matching\n",
    "check_anchor_matching(dataset=train_dataset, model=model, thr=4., imgsz=args.img_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9038253c-3fda-481f-ba6b-e5ccec1e3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_processing.yolov7.dataset.anchors import best_possible_recall_metric, kmean_anchors\n",
    "from video_processing.yolov7.models.common import check_anchor_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b527cfb-50ea-4c06-8670-40959dce7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=train_dataset\n",
    "thr=4.\n",
    "imgsz=args.img_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d802c-fcf6-4d99-85e3-78121debf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anchor_matching(dataset, model, thr=4., imgsz=640):\n",
    "    '''\n",
    "    Check whether anchors is not thrx bigger or smaller than boxes in training data. If not, attempt to update it using genetic-evolved kmean\n",
    "    Args:\n",
    "        dataset: contains normalized box information [normalized box center (x,y) and normalized width/height (w,h)]. It must also\n",
    "            contain width/heights of all images (used to denormalized boxes)\n",
    "        model: contain detection module with anchors, anchor_grid, and stride information as input anchors to check and to update \n",
    "            if needed\n",
    "        thr (float): largest factor that allows anchors to be bigger/smaller than boxes in training data\n",
    "        imgz (int): input image size for traing model (not original image size)\n",
    "    '''\n",
    "    # get detection module\n",
    "    module=model.model[-1]\n",
    "    # make the maximum size = imgsz while keeping the aspect ratio consistent\n",
    "    shapes=imgsz*dataset.image_sizes/dataset.image_sizes.max(1, keepdims=True) # Nx2 where N is the number of images\n",
    "    scale=np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1)) # Nx1 augment scale\n",
    "    # change normalized width,height to width/height in pixel unit using scaled_shape then stack along number of boxes to Nx2\n",
    "    boxes_width_heights=torch.from_numpy(np.concatenate([s[None,:]*l[:,3:] for s, l in zip(shapes*scale, dataset.labels)], axis=0)).float()\n",
    "\n",
    "    # nlx1xnax1x1x2 -> Mx2 width and height of each anchors\n",
    "    anchors_width_heights=module.anchor_grid.clone().cpu().view(-1, 2)\n",
    "    bpr,n_anchors=best_possible_recall_metric(boxes_wh=boxes_width_heights, anchors_wh=anchors_width_heights, threshold=thr)\n",
    "    print(f'In dataset.anchors.check_anchor_matching anchors/target={n_anchors:.2f}, Best possible recall (BPR): {bpr:.4f}')\n",
    "\n",
    "    if bpr>0.98: return\n",
    "        \n",
    "    # recompute anchors\n",
    "    n_anchors=module.anchor_grid.numel()//2 # divide by 2 because this contain both width and height\n",
    "    try: new_anchor_width_heights=kmean_anchors(dataset=dataset, n_anchors=n_anchors, img_size=imgsz, thr=thr, n_generations=1000, verbose=False)\n",
    "    except Exception as e: print(f'Error: {e}')\n",
    "    new_bpr=best_possible_recall_metric(boxes_wh=boxes_width_heights, anchors_wh=new_anchor_width_heights, threshold=thr)[0]\n",
    "    \n",
    "    if new_bpr<bpr: \n",
    "        print(f'In dataset.anchors.check_anchor_matching new anchors have lower bpr of {new_bpr} compared to {bpr}--used old one')\n",
    "        return\n",
    "\n",
    "    new_anchor_width_height_tensor=torch.from_numpy(new_anchor_width_heights).to(device=module.anchors.device, dtype=module.anchors.dtype)\n",
    "    module.anchor_grid=new_anchor_width_height_tensor.clone().view_as(module.anchor_grid)\n",
    "    print(module.anchors.shape, module.stride.shape)\n",
    "    # normalize the anchors from image grid to feature cell grid\n",
    "    module.anchors=new_anchor_width_height_tensor.clone().view_as(module.anchors)/module.stride.to(module.anchors.device).view(-1,1,1)\n",
    "    check_anchor_order(module)\n",
    "    print('In dataset.anchors.check_anchor_matching:: New anchor width/height has been estimated. Please update configuration file')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4da42494-e3e4-42ee-bc35-731ab85c508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes_width_heights  torch.Size([802, 2]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# get detection module\n",
    "module=model.model[-1]\n",
    "# make the maximum size = imgsz while keeping the aspect ratio consistent\n",
    "shapes=imgsz*dataset.image_sizes/dataset.image_sizes.max(1, keepdims=True) # Nx2 where N is the number of images\n",
    "scale=np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1)) # Nx1 augment scale\n",
    "# change normalized width,height to width/height in pixel unit using scaled_shape\n",
    "# then stack along number of boxes to Nx2\n",
    "boxes_width_heights=torch.from_numpy(np.concatenate([s[None,:]*l[:,3:] for s, l in zip(shapes*scale, dataset.labels)], axis=0)).float()\n",
    "print('boxes_width_heights ', boxes_width_heights.shape, boxes_width_heights.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97bc2542-1f44-496e-9d42-ad2cb80b42df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors_width_heights  torch.Size([12, 2]) torch.float32\n",
      "anchors/target=5.68, Best possible recall (BPR): 0.9975\n"
     ]
    }
   ],
   "source": [
    "# nlx1xnax1x1x2 -> Mx2 width and height of each anchors\n",
    "anchors_width_heights=module.anchor_grid.clone().cpu().view(-1, 2)\n",
    "print('anchors_width_heights ', anchors_width_heights.shape, anchors_width_heights.dtype)\n",
    "bpr,n_anchors=best_possible_recall_metric(boxes_wh=boxes_width_heights, anchors_wh=anchors_width_heights, threshold=thr)\n",
    "print(f'anchors/target={n_anchors:.2f}, Best possible recall (BPR): {bpr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db050d4a-ca5c-401f-85c9-7d2c8c1d915d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_anchors  12\n",
      "Running Kmeans for 12 on 802 boxes\n",
      "threshold=0.25: 0.9925 best possible recall, 5.45 anchors passes the threshold\n",
      "n_anchors=12, img_size=1280, metric_all=0.279/0.716 mean/best, past_threshold 0.473-mean: (75,148), (866,462), (466,297), (50,69), (25,31), (178,357), (317,580), (1195,741), (149,100), (272,183), (583,924), (117,226)\n",
      "new_anchor_width_heights  (12, 2)\n",
      "new_bpr  tensor(0.9963)  bpr  tensor(0.9975)\n"
     ]
    }
   ],
   "source": [
    "# if bpr<0.98: recompute anchors\n",
    "n_anchors=module.anchor_grid.numel()//2 # divide by 2 because this contain both width and height\n",
    "print('n_anchors ', n_anchors)\n",
    "try: new_anchor_width_heights=kmean_anchors(dataset=dataset, n_anchors=n_anchors, img_size=imgsz, thr=thr, n_generations=1000, verbose=False)\n",
    "except Exception as e: print(f'Error: {e}')\n",
    "print('new_anchor_width_heights ', new_anchor_width_heights.shape)\n",
    "new_bpr=best_possible_recall_metric(boxes_wh=boxes_width_heights, anchors_wh=new_anchor_width_heights, threshold=thr)[0]\n",
    "print('new_bpr ', new_bpr, ' bpr ', bpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb2d6667-2e1e-4dbf-b3c4-4b09c2829421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]), torch.Size([4]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if new_bpr>bpr:\n",
    "new_anchor_width_height_tensor=torch.from_numpy(new_anchor_width_heights).to(device=module.anchors.device, dtype=module.anchors.dtype)\n",
    "module.anchor_grid=new_anchor_width_height_tensor.clone().view_as(module.anchor_grid)\n",
    "print(module.anchors.shape, module.stride.shape)\n",
    "# normalize the anchors from image grid to feature cell grid\n",
    "module.anchors=new_anchor_width_height_tensor.clone().view_as(module.anchors)/module.stride.to(module.anchors.device).view(-1,1,1)\n",
    "check_anchor_order(module)\n",
    "print('New anchor width/height has been estimated. Please update configuration file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3aad6013-30fa-49d6-843c-7e6637874b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[  57.7727,  150.0484]]],\n",
       "\n",
       "\n",
       "          [[[ 932.3475,  430.9550]]],\n",
       "\n",
       "\n",
       "          [[[ 414.1675,  257.0540]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[  36.0162,   57.3181]]],\n",
       "\n",
       "\n",
       "          [[[  19.9298,   25.1142]]],\n",
       "\n",
       "\n",
       "          [[[ 185.3887,  301.4194]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 322.3356,  530.1092]]],\n",
       "\n",
       "\n",
       "          [[[1140.8296,  818.9543]]],\n",
       "\n",
       "\n",
       "          [[[ 108.1660,   87.4579]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 196.4072,  152.4655]]],\n",
       "\n",
       "\n",
       "          [[[ 539.0780,  816.8771]]],\n",
       "\n",
       "\n",
       "          [[[  92.1634,  178.1084]]]]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.anchor_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26703619-d2ca-4399-aad2-378ae2ef1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_anchor_order"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:op_cv]",
   "language": "python",
   "name": "conda-env-op_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
